# 数据压缩入门
**定义：**
数据压缩是指在不丢失有用信息的前提下，缩减数据量以减少存储空间，提高其传输、存储和处理效率，或按照一定的算法对数据进行重新组织，减少数据的冗余和存储的空间的一种技术方法。

## 数据压缩的两个思路
* 减少数据中不同符号的数量（即让“字母表”尽可能小）；
* 用更少的位数对更常见的符号进行编码（即最常见的“字母”所用的位数最少）。

## 熵
表示整个数据集所需要的最少二进制位数。
下面的公式用来计算一个集合的熵H(S)：
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/7C7A5CE3-F9D2-4659-ADE9-9B60BA412F7E.png)
突破熵的关键在于，通过利用数据集的结构信息将其转换为一种新的表示形式，而这种新表示形式的熵比源信息的熵小。

## 一套完整的压缩算法，可分为如下几部分：
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/ED8D4E5D-4403-4E33-95B0-19D595E5AF45.png)

![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/EAD99636-0D46-477E-8684-493C90D9B773.png)

## 文本压缩分类
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/C4C5DB52-306F-4896-9DCE-F98F71FA41F2.png)

## 模型
在压缩程序中，用来处理输入信息，计算符号的概率并决定输出哪个或哪些代码的模块叫做模型。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/ECC8ED70-8B3F-4D9B-B56D-B31F3806E2D7.png)

## 霍夫曼编码
霍夫曼编码
最小方差霍夫曼码的优点：
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/FD84671A-2D7A-40B2-9AE8-FF9E80069EB4.png)

## VLC
VLC：variable-length codes，符号分配变长编码。
### VLC编码步骤：
一般来说，对数据进行VLC通常有3个步骤：
(1) 遍历数据集中的所有符号并计算每个符号的出现概率。
(2) 根据概率为每个符号分配码字，一个符号出现的概率越大，所分配的码字就越短。
(3) 再次遍历数据集，对每一个符号进行编码，并将对应的码字输出到压缩后的数据流中。下面来对每一个步骤进行一些讲解。
### VLC设计原则：
在设计VLC集的码字时，必须考虑两个原则：
* 是越频繁出现的符号，其对应的码字越短；
* 是码字需满足前缀性质（在某个码字被分配给一个符号之后，其他的码字就不能用该码字作为前缀）。
### 几种VLC类型：
1.二进制编码
2.一元码
3.Elias gamma编码
4.Elias delta编码
### VLC主要存在问题：
VLC主要存在以下几个问题，因而只能用于表示压缩数据流，没有其他应用。
* 它们不按字节 / 字 / 整型对齐。
* 对于大的数值[~插图~]，为了方便解码，其码字长度的增长速度一般会超过[~插图~]个二进制位。
* 解码的速度很慢（每次只能读取一个二进制位）。
### 使用VLC注意：
LC编码方法是根据某个数值期望的出现频率来为该值分配码字的。
因此，每种VLC编码方法，对于数据集中的各个符号如何分布，都有相应的期望。
因此，为数据集选择适当的VLC编码方法，关键在于使VLC背后的概率模型与该数据集匹配。
如果偏离了这个方向，最终得到的可能会是更大的数据流。

## 统计编码
统计编码算法通过数据集中符号出现的概率来进行编码使结果尽可能与熵接近。
### 哈夫曼编码
哈夫曼编码能生成理想VLC（即码字的平均长度等于符号集的熵）的唯一情形是，各个符号的出现概率等于2的负整数次幂（即是1/2、1/4或1/8这样的值）。这是因为哈夫曼方法会为给定符号集中的每个符号都分配一个整数二进制位长的码字。
### 算术编码
算术编码的工作原理是将字符串转换为一个数，与字符串相比，表示这个数需要的二进制位数要少一些。
算术编码首先会创建[~0,1)[插图~]这样的数值区间，然后再通过数据流中符号出现的概率对这一区间进行细分。比如说，如果A出现的概率为25%，那么为A分配的区间就是[0,0.25)，B出现的概率为10%，那么B对应的区间则为[0.25,0.35)，以此类推，如下图所示。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/BADFF137-C34A-4768-AECD-DD558D6AD7D3.png)
当编码器读取一个符号时，它就会找到这个符号对应的区间。例如，如果读取字符A，那么用到的区间就是[0.0,0.25)。在读取一个符号以后，编码器将继续细分这个取值区间，并按符号的出现比例进行细分。
例如，如果遇到的输入流中有3个A，那么编码器将对A的取值区间细分3次，如下图所示。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/DE091D4B-426C-47CA-8354-B61C22877D50.png)
总的来说，每个符号都会以递归的方式再次细分取值区间直到读完整个输入流为止。之后，就得到最终的取值区间，比如[0.253212,0.25421)。而最终输出的数值，也在这一取值区间内。例如，上面所举的输入为AAA的例子，其最终输出值就在[0,0.015625)这个区间之内。

### ANS
ANS：asymmetric numeral systems，非对称数字系统。
(1) 根据符号的出现频率对数值区间进行细分。
(2) 创建一张表，将子区间与离散的整数值关联起来。
(3) 每个符号都是通过读取和响应表中的数值来处理的。
(4) 向输出流中写入可变的二进制位位数。这一算法独有的两个部分在第(2)步（子区间与整数值关联的表）和第(4)步（可变的二进制位位数）中。

**使用ANS的压缩工具**：
ZHuff、LZTurbo、LZA、Oodle和LZNA
**FSE：**
FSE：Finite State Entropy，有限状态熵。是ANS中一种更重性能的版本，只使用加法、掩码和移位运算。
一款名为LZFSE的GZIP变种，就是以FSE实现。

## 自适应统计编码
上一节介绍的统计编码算法，在编码开始之前都需要遍历一次数据，以计算出各符号出现的概率。
数据中总会存在某种类型的局部偏态（locality-dependent skewing）[~插图~]，将某些符号、想法或者单词集中在数据集的某个子区间里。
这种具有适应数据流熵的局部特性能力的统计编码算法，通常被称为“动态”或“自适应”统计编码算法。这些算法变体构成了大多数重要的、高性能的、高压缩率的多媒体数据流（如图片、视频及音频）压缩算法的基础。
### 自适应VLC编码
### 自适应算术编码
### 自适应哈夫曼编码

## 字典转换
#### 字典转换的工作方式
给定源数据流，首先构建出单词字典（而不是符号字典），然后再将统计压缩应用到字典中的单词上。
字典转换并非是要去替代统计编码，相反，它只是你先应用到数据流上的一个转换，这样统计编码算法就能更有效地对其编码。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/CD33793B-C78A-43EF-9DFA-CDCAD1FA8CB4.png)
可以通过被称为分词（tokenization）的过程找出这些“单词”，即分析一组数据并从中找出理想的“单词”。
### LZ算法
**LZ77和LZ78的衍生算法：**
GIF图像格式中使用的LZW（即Lempel-Ziv-Welch）算法；
应用于7-Zip、xz等压缩工具的LZM（即Lempel-Ziv-Markov chain）算法。
这些算法也同样应用于DEFLATE这样的压缩算法中，而DEFLATE又应用于PNG图像格式、PKZIP、GZIP等压缩工具及zlib库中。
#### LZ算法的工作原理
LZ算法尝试通过在读取的字符串中寻找当前单词的匹配来分词。与读取一组符号然后向后查找它是否重复出现不同，LZ算法向前查找当前单词是否出现过。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/F8C4BFC8-7864-4DD0-A9DF-7D2BB17D88A0.png)
1）搜索缓冲区
LZ算法的工作原理是将数据流分成如下两部分：
* 数据流的左半部分通常被称为“搜索缓冲区”（searchbuffer），包含的是已经读过并处理过的符号。
* 数据流的右半部分则被称为“先行缓冲区”（look aheadbuffer），包含的是将要编码的符号。
因此，当前“读取”的位置就位于两个缓冲区之间：
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/07554F5E-5907-4FEE-9930-53ECA3414663.png)
2）找出匹配
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/7924FDD1-15B9-4EDE-97FB-A38D32A3B2EB.png)
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/274C064D-A8A5-45E2-97ED-3F4708BC8076.png)
3）滑动窗口
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/D28E24ED-02BF-4354-8669-35D1DEC6728B.png)
一般来说，搜索缓冲区滑动窗口的长度大概为几万个字节，而先行缓冲区的长度则只有几十个字节。

4）用记号标记匹配
当匹配最终确定下来，编码器就会生成一个固定长度的记号并将它写入输出流。
该记号主要由两部分组成：偏移量和长度。
* 偏移量该值表示的是搜索缓冲区中匹配单词的起始位置，从当前位置向前数。
* 长度值该值表示的是匹配单词的长度。在本例中，匹配单词的长度为4（即包含4个符号）。
5）没有找到匹配时
在一些情况下，无法在搜索缓冲区中找到先行缓冲区中出现符号的匹配。
这种情况下，需要输出一些信息来表示这个新出现的单词，这样解码器才能正确地还原它。
因此，需要对输出的记号进行修改，表明输出的是字面值，这样解码器就能读取并恢复源数据流。
不过，怎样构造该记号完全取决于具体的LZ算法实现。
一种最基本的做法是，将修改后的记号表示为三部分，前两部分与前面介绍的相同，还是偏移量和长度值，只不过取值都为0，即[~0,0~],最后一部分则是符号的字面值
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/BB80279E-27E9-4DA3-A2B2-1BD05026D78F.png)

####  压缩LZ算法的输出
LZ算法真正吸引人的地方还在于它可以和统计编码结合使用。
可以将记号中的偏移量、长度值以及字面值分开后，再按照类型合并，组成单独的偏移量集、长度值集和字面值集，然后再对这些数据集进行统计压缩。

#### LZ算法的变体
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/FC95B4D5-E16E-469E-BCA5-D3C028934B74.png)

1.LZ77
LZ77基本算法（有时也被称为LZ1算法）的工作原理，与前面介绍的大致相同。唯一的区别是，它会将先行缓冲区中下一个符号的字面值作为第三个值输出。
2.LZSS
LZ77与LZSS的主要差别是：
在LZ77算法中，字典引用可能会比其替换的字符串还长；而在LZSS中，如果被替换的字符串长度值小于“收支平衡”点，那么这样的引用就会被忽略。
此外，LZSS还会用一个标志位来区分后面的数据是字面值还是偏移量–长度值二元组这样的引用。
很多流行的压缩工具比如PKZip、ARJ、RAR、ZOO和LHarc使用LZSS算法，并将其作为主要的压缩算法。值得一提的是，Game Boy Advance游戏机的BIOS就自带解码改进后的LZSS格式补丁等功能。
3.LZ78或LZ2
LZ算法系列的核心算法最早发表于1977~1978年，这两种算法有时也被称为LZ1算法和LZ2算法。
LZ78算法的工作原理与前面描述的基本相同，不过它不用距搜索缓冲区结尾的偏移量来指示匹配的位置，而是根据输入流创建字典然后再引用。
4.LZW算法
LZW（Lempel-Ziv-Welch）算法是由Terry Welch于1984年提出的，它采用了LZ78算法的思想，其工作原理如下。
(1) LZW算法将字典初始化为包含所有可能的输入字符，如果用到了清空和停止符号(clear and stop codes），那么这两个符号也包括在其中。
(2) 该算法扫描输入字符串以寻找更长的连续子串，直到它发现该子串在字典中不存在。
(3) 当发现这样的子串时，去掉它的最后一个符号（这样它就变成当前字典中最长的子串），然后从字典中找出其索引并输出。
(4) 将该子串（此时包括最后一个符号）加入字典作为新的词条。
(5) 将该子串的最后一个符号作为起点，重新扫描下一个子串。
用这种方法，连续更长的子串就会作为新的词条加入字典，同时也让后续字符串编码为单值输出成为可能。
该算法最适用于那些连续出现重复的数据，因为在数据的初始部分，基本看不到什么压缩，但是随着数据的增多，压缩率逐渐趋于最大值。
LZW算法成为首个在计算机中广泛采用的通用数据压缩方法。公共领域程序“compress”也采用了LZW算法，并在1986年前后就基本成为UNIX系统的标准应用程序。此后，“compress”就从很多UNIX分发中消失了，一方面是因为它侵犯了LZW的专利权，另一方面是因为GZIP的压缩率更高（GZIP使用的是基于LZ77的DEFLATE算法）。

## 上下文数据转换
上下文转换：给定一组相邻的符号集，对它们进行某种方式的变换使其更容易压缩。我们通常称这样的变换为“上下文变换”（contextual transform），因为在思考数据的理想编码方式时，这些方法考虑到了邻近符号的影响。
### RLE
RLE：run-length encoding 行程编码
RLE主要针对的是连续出现的相同符号聚类的现象，它会用包含符号值及其重复出现次数的元组，来替换某个符号一段连续的“行程”（run）。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/7D40F81F-6FAE-4E53-BD91-C465F46A3FC1.png)
RLE算法最适用于大多数符号都连续重复出现的数据集。
虽然RLE在现代压缩工具中用得并不多，但还是有人在研究更高效的RLE。例如，最近就出现了一种新的RLE压缩工具TurboRLE，号称是速度最快、效率最高的RLE编码器。

### 增量编码
增量编码（delta coding），其实就是将一组数据转换为各个相邻数据之间的相对差值（即增量）的过程。它背后的思想是，给定一组数据，相关的或相似的数据往往会集中在一起。如果这样，有了两个相邻值之间的差，就可以用其中一个值以及该差值来表示另外一个值。
它依靠的是相邻性，所以最适用于处理时间序列数据（比如每10秒检测一次温度的传感器所产生的数据），以及音频和图像数据这类多媒体数据，因为这类数据中邻近的数据之间存在着时间上的关联。
#### XOR增量编码
减法增量编码算法的问题是，结果中可能会出现负数，进而产生各种问题。负数不仅在存储的时候需要额外的二进制位，此外还可能会增大数据的变化范围。
可以通过使用按位异或运算（bitwise exclusive OR，XOR）代替减法运算来解决这一问题。
#### 参考系增量编码
参照系方法通过让其他数减去最小的数。
“参照系”（frame of reference，FOR）中那个“参照数”（frame）的选取，与将转换恰当地应用到数据集上有关，因此需要将数据集细分为更小的数据组。
#### 修正的参照系增量编码PFOR
PFOR：Patched Frame of ReferenceDelta Coding 参照系增量编码
**工作原理：**
(1)选择一个位宽度[插图]。
(2)遍历数据并用[插图]位对每个值编码。
(3)当遇到的值所需的编码位数大于[插图]时，将这样的离群值存储在单独的位置。

PFOR的神奇之处就在于其对离群值的处理：
(1)考虑如下例子：
[1,2,10,256]增量编码后，得到：[1,2–1,10–2,256–10] = [1,1,8,246]。
(2)用最简单的形式将这组数据分成两部分，即编码需要的二进制位数不大于b的以及大于b的。当b=4时，得到[1,1,8][246]这两组数据。
(3) 用4位对第一组中的每个数编码，用8位对第二组中的离群值编码。
(4)为了让离群值能合并到源数据列中，需要知道它的位置信息，因此得到的结果为[1,1,8][246][3]。
在解码时，我们需要取出离群值并将它们放回源数据流，然后再进行增量编码的逆操作。
这里要解决的两个问题：
1.选择合适的b值：
我们的目标很明确，就是选择一个合适的b值，使大多数值在编码时需要的位数不超过b，并且可以通过它识别出那些离群值。
通常可以逐步做到这一点。可以从b=1开始试验，看看数据集中有多少数小于2^1，如果数据集中90% 的数小于2，那么就将b设置为1。否则，令b的值为2，再看看数据集中有多少数小于2^2。如有必要，可以重复这一过程，比如令b的值为3并看看有多少值小于2^3，直到数据集中90% 的值满足条件。
2.怎样处理离群值：
PFOR的有趣之处在于，除了得到修改后的增量信息外，我们还得到了第二个数据集，它表示的是那些离群值。
第二个数据集中的值变化范围很大，通常很难直接压缩。根据原始论文Super-Scalar RAM-CPU Cache Compression 的叙述，PFOR没有将整个的离群值原样扔进一个新的列表中，而是将最低的“b”位留在源数据流中，并将剩下的部分存储在离群值列表中。
#### 压缩增量编码后的数据
如果增量编码能做到以下两点，那么我们就可以认为它生成的数据更容易压缩：
* 将数据集中的最大值变小，因此缩小了数值的变化范围；
* 生成了许多重复值，可以让统计压缩的效率更高。
一般来说，将增量编码后的结果交给统计编码算法处理，会产生良好的压缩效果。
**增量编码对文本不会那么有效。英语文本使用LZ算法可能做得更好。**

### MTF
MTF：move-to-front coding 前移编码
上下文数据转换背后的基本思想是：数据的排列次序中包含着一些有助于编码未来符号的信息，前移编码（move-to-frontcoding，MTF）利用的也是这样的信息，且MTF考虑更多的是在较短的窗口内某个特定符号的出现次数。
MTF反映了如下的预期：如果一个符号在输入流中出现了，那么它很有可能会出现多次，或者至少短时间内会成为常见的符号。MTF是局部自适应的，因为它会根据输入流中局部区域符号的出现频次进行调整。
MTF过程：
它是通过另外管理一组数据来工作的，其中包含的是数据集中所有不同的值，我们称为SortedArray。
当从数据流中读取一个值时，我们会找出该值在SortedArray中的索引并将此索引值输出，然后更新SortedArray，将该值移到最前面，即让其索引变为0。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/FBD76B73-63B0-40D0-8524-01A06A8172E5.png)
#### 消除捣乱符号的影响
MTF存在的一个问题是，有一些捣乱的符号会打乱前面存在的符号流。这个问题比较严重，因为会极大地破坏编码，而且在真实数据中普遍存在。一种解决方法是，不是一读到某个符号就将它移到最前面，而是采取一些探索式方法慢慢地将它移到最前面。
1.向前移动k个位置
有两种确定k值的简单方法：
* 令k=n（n为符号的个数），原始的 MTF就是这种选择；
* 令k=1，即某个符号被读取一次，它的位置就前移一位。
2.出现C次再移动
采用这种方法时，SortedArray中的元素只有在输入流中出现过C次之后（并非必须连续出现），才会移动到最前面。SortedArray中的每个元素都有一个计数器，记录该元素出现的次数。这样我们就可以为符号移动到最前面设定一个出现次数的阈值。当应用到文本上时，我们就可以通过最终生成的SortedArray，来反映所编码的语言各个字母的使用频率。
#### 压缩MTF
MTF生成的输出流的熵通常要比源数据流小，这让它的输出成为传递给统计压缩器进行进一步压缩的首选对象。由于MTF生成的输出流中有很多的0和1，因此简单的统计编码算法就可以工作得很好。
实际上，MTF是最简单的动态统计转换形式之一。

### BWT
BWT：Burrows-Wheelertransform 伯罗斯–惠勒变换
BWT通过打乱数据流次序来让重复的子串聚集在一起。这一操作本身不能压缩数据，却可以为后续的压缩系统提供转换好的数据流，方便压缩。
#### BWT的工作原理
要开始BWT的“变换”工作，需要先创建一张表，其中包含输入流的所有移位排列。
接下来，BWT会对表中的每一行按字典顺序排序。
这样由最后一列字符组成的排列，与源字符串相比，能更好地将相同的字符聚集起来。
观察排序后的表格，记录最后一列字符组成的排列得到的字符串在输入字符串中的索引。在BWT的解码阶段，我们需要该索引值，因为它将使我们从更易压缩的排列回到源字符串上。
#### BWT的逆操作
BWT最引人注目的特点并不在于它能生成更易压缩的输出（普通排序也能做到这一点），而在于只需要极小的数据开销，它所进行的变换操作就是可逆的（reversible）。
#### 具体的实现
不能在50GB大小的文件上进行BWT。
这种置换变换的工作方式就决定了，我们每一行都需要存储50GB大小的符号（每一列也是50GB大小），并且按行依次左移一位符号。整个符号矩阵需要的空间太大了。
因此，我们通常将BWT称为块排序变换，具体实现时，它会将整个文件分为许多1 MB大小的数据块，然后在每个数据块上分别应用该算法。这样一来，大多数现代设备就能满足该算法对内存的要求，同时该算法也能获得较好的性能。
#### 压缩BWT后的数据
显然，BWT本身不压缩数据，它只是转换数据。为了让BWT真正起作用，还需要应用其他的转换来生成熵值更小的数据流，然后再对其压缩。
最常见的算法是将BWT的输出作为MTF的输入，经过处理后接着用统计编码算法处理。这基本上就是BZIP2的内部工作原理。


## 数据建模
### 马尔可夫链
马尔可夫链是一种离散的随机过程，其未来的状态只取决于现在，而与过去的历史无关。
#### 马尔可夫链与压缩
马尔可夫链这一概念能很好地融入现有的模型，因为可以认为统计编码算法就是一阶马尔可夫链。有了数据流中各符号出现的概率表，我们就能为其分配相应的码字。
通过为每个在前面出现的符号增加一张符号码字对应表，我们就可以创建二阶马尔可夫链。
当应用到压缩上，马尔可夫链可以让我们用更少的二进制位数对相邻的符号编码。
如果遇到更多的符号和更长的输入流，我们就可以建立更多阶的上下文。
#### 实际的实现
值得指出的是，没有人真正地使用马尔可夫链来压缩数据，至少不会用前面说的方法来压缩。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/5A6F2689-D03A-4346-B32D-162304905708.png)

### 部分匹配预测算法
要使马尔可夫链算法变得实用，就必须要解决内存消耗问题与计算性能问题，即使用最佳链来编码。
PPM：prediction by partial matching 部分匹配预测算法。
该算法在内存消耗与计算性能方面表现都还不错。与马尔可夫链类似，PPM算法同样通过前N个符号的上下文来决定第N+1个符号最有效的编码方式。
给定输入流中的当前符号，PPM算法会向前扫描N个符号，并根据前N个符号的上下文来决定当前符号的出现概率。如果在N个符号的上下文中，当前符号的出现概率为0，PPM算法就会将上下文符号的个数减少为N-1。如果没有在任何上下文中发现匹配，就会做出固定的预测。
#### 单词查找树
要实现PPM算法，遇到的首要问题是如何创建一种数据结构，可以将从输入流中读取的每个字符的所有上下文（0～N阶）存储起来，并且在需要时能快速定位到。在简单的情况下，可以通过一种被称为单词查找树的特殊树结构来实现这样的需求，这种树的每个分支都表示一种上下文。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/99731E49-BA12-4253-9A5F-0DA1FEF721EA.png)
#### 字符的压缩
除了能提供高效的存储以及快速提取子字符串外，单词查找树的每一层都会记录字符出现的次数。有了这些数据，统计编码算法就能构建出字符出现的概率表，并根据概率表为每个字符分配相应的码字。
![](%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E5%85%A5%E9%97%A8/434C1CB0-4CBC-492E-959C-9B2F16C6CC3F.png)
#### 选择一个合理的N值
那么N值应该是多少呢？ PPM算法会选择一个N值，然后再根据这样的上下文长度去寻找匹配。如果没有找到匹配，就会选择更短的上下文继续寻找。这样看来，似乎上下文越长（也就是N的取值越大），预测的效果越好。
然而，大多数PPM算法的实现在综合考虑所需内存、处理速度以及压缩率后，将N的值设定为5或者6。
也有一些PPM算法的变体，例如PPM*，尝试着使N的取值变大，并且是变得非常大。这样做，不仅需要一种新的查找树结构，而且需要的计算资源也远比PPM多，但其结果则比PPM算法好，能多节省约6% 的存储空间。
#### 处理未知的符号
PPM算法（模型）的大部分优化工作是在处理输入流中那些之前没有出现过的字符。显而易见的处理方法是通过创建“从没见过”的符号来触发转义序列（escape sequence），但是那些没有见过的符号应该赋什么样的概率值呢？这通常被称为零频问题（zero-frequency problem）。
有一种PPM算法的变体使用拉普拉斯估计（Laplaceestimator），赋给所有“从没见过”的符号相同的伪计数值1。还有一种被称为PPMD的变体是这样处理这一问题的，“从没见过”的符号每使用一次，伪计数值就加1。（换句话说，PPMD算法是这样估算新出现符号的概率的，即新符号的概率等于不同符号的个数与观察到的所有符号的出现次数之比。）
PPMZ算法的处理更有意思。刚开始时它的处理方式与PPM* 相同，都试图在N阶上下文下找出当前符号的匹配。当找不到这样的匹配时，它就会换上完全不同的算法局部阶估计法（Local-Order-Estimator），而使用的还是基本的PPM模型，只不过预测的算法完全不同。

###  上下文混合算法
对PPM算法的改进，让我们有了新的数据压缩算法，PAQ 系列算法。特别是在PPMZ算法中，对于符号如何去响应匹配，人们尝试了多种类型的上下文。
随着时间的推移，这一概念逐渐被称为上下文混合算法（context mixing），即为了找出给定符号的最佳编码，我们会使用两个或者更多的统计模型。
#### 模型的类型
我们所说的模型其实就是用来识别和描述符号之间的关系。通过对数据的建模，我们就能对数据中包含的各种属性了解得更多，因而也就越能描述好当前的符号。
实际上，模型可以有很多种，需要处理的数据类型不同，模型也会不同。
作为上下文混合算法的先驱压缩器之一，PAQ包含以下模型：
* n元语法（n-grams），这里上下文是指在被预测符号之前的n个字符（与 PPM算法相同）；
* 整词n元语法（whole-word n-grams），忽略大小写和非字母字符（对文本文件很有用）；
* “稀疏”上下文，例如，被预测符号之前的第二个和第四个字节（对某些二进制文件很有用）；
* “模拟”上下文，由前面的 8位或者 16位字节的高字节位组成（对多媒体文件很有用）；
* 二维上下文（对图像、表和电子表格很有用），行的长度由找出的重复字节模式的步长决定；
* 只针对特定文件类型的特殊模型，例如 x86可执行文件，BMP、TIFF或者 JPEG格式的图片。
#### 混合的类型
将不同模型的输出结合起来有以下两种方法：
一种是线性混合（linear mixing），它是将各个模型的预测值加权平均的过程，最终的值则取决于证据权重。
第二种是逻辑混合。逻辑混合使用了神经网络来更新权重，而更新的依据则是哪个模型在过去给出了最准确的预测。逻辑混合的缺点是，在进行数据压缩时，它需要消耗大量的内存，同时运行的时间也较长。

## 综合通用无损压缩算法
* deflate:
先用LZ77（或 LZSS）算法预处理，然后用霍夫曼编码对压缩后的 literal、length、distance 编码优化。deflate是如今最流行的通用压缩算法之一。
* bzip2：
涉及多种算法，主要流程包括先使用 RLE游程编码对原始数据进行处理，然后通过 BWT转换（可逆的处理一段输入数据使得相同字符连续出现的次数最大化），再用 MTF转换，然后再次使用RLE游程编码处理，接下来还会进行霍夫曼编码以及一系列相关处理，较为复杂，速率劣于DEFLATE但压缩率更高。
* LZMA：
实现了LZ77修改版以位（bit）而非字节（byte）为单元级别的操作，并通过马可夫链实现字典索引，速率和压缩率优于bzip2，另有多线程优化的版本LZMA2。
* Brotli： 
基于LZ77算法的一个现代变体，使用了霍夫曼编码和二阶上下文建模，使用了预定义的120千字节字典包含超过13000个常用单词、短语和其他子字符串，预定义的算法可以提升较小文件的压缩密度。总体速率接近于DEFLATE且压缩率接近于LZMA。
