# 系统
### 图灵机的工作方式
![](%E7%B3%BB%E7%BB%9F/FC36E408-8321-47FC-B706-8A3EC0E1274C.png)
* 有⼀条「纸带」，纸带由⼀个个连续的格⼦组成，每个格⼦可以写⼊字符，纸带就好⽐内存，⽽纸带上的格⼦的字符就好⽐内存中的数据或程序；
* 有⼀个「读写头」，读写头可以读取纸带上任意格⼦的字符，也可以把字符写⼊到纸带的格⼦； 
* 读写头上有⼀些部件，⽐如存储单元、控制单元以及运算单元：
1、存储单元⽤于存放数据；
2、控制单元⽤于识别字符是数据还是指令，以及控制程序的流程等；
3、运算单元⽤于执⾏运算指令；

### 线路位宽与CPU位宽
线路位宽：
为了避免低效率的串⾏传输的⽅式，线路的位宽最好⼀次就能访问到所有的内存地址。 CPU 要想操作的内存地址就需要地址总线，如果地址总线只有 1条，那每次只能表示  「0 或 1」这两种情况，所以 CPU ⼀次只能操作 2 个内存地址，如果想要 CPU 操作 4G 的内存，那么就需要 32 条地址总线，因为     2 ^ 32 = 4G 。
CPU位宽：
32 位和 64 位 CPU 最主要区别在于⼀次能计算多少字节数据：
32 位 CPU ⼀次可以计算 4 个字节；
64 位 CPU ⼀次可以计算 8 个字节；
这⾥的 32 位和 64 位，通常称为 CPU 的位宽。
#### 64 位相⽐ 32 位 CPU 的优势在哪吗？64 位 CPU 的计算性能⼀定⽐ 32 位 CPU ⾼很多吗？
64 位相⽐ 32 位 CPU 的优势主要体现在两个⽅⾯：
* 64 位 CPU 可以⼀次计算超过 32 位的数字，⽽ 32 位 CPU 如果要计算超过 32 位的数字，要分多步骤进⾏计算，效率就没那么⾼，但是⼤部分应⽤程序很少会计算那么⼤的数字，**所以只有运算⼤数字的时候，64 位 CPU 的优势才能体现出来，否则和 32 位 CPU 的计算性能相差不⼤。**
* 64 位 CPU 可以寻址更⼤的内存空间，32 位 CPU 最⼤的寻址地址是 4G，即使你加了 8G ⼤⼩的内存，也还是只能寻址到 4G，⽽ 64 位 CPU 最⼤寻址地址是 2^64 ，远超于 32 位 CPU 最⼤寻址地址的 2^32 。
#### 你知道软件的 32 位和 64 位之间的区别吗？再来 32 位的操作系统可以运⾏在 64 位的电脑上吗？64 位的操作系统可以运⾏在 32 位的电脑上吗？如果不⾏，原因是什么？
64 位和 32 位软件，实际上代表指令是 64 位还是 32 位的：
* 如果 32 位指令在 64 位机器上执⾏，需要⼀套兼容机制，就可以做到兼容运⾏了。但是如果 64 位指令在 32 位机器上执⾏，就⽐较困难了，因为 32 位的寄存器存不下 64 位的指令；
* 操作系统其实也是⼀种程序，我们也会看到操作系统会分成 32 位操作系统、64 位操作系统，其代表意义就是操作系统中程序的指令是多少位，⽐如 64 位操作系统，指令也就是 64 位，因此不能装在32 位机器上。
总之，硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽。

### 存储器的层次结构
![](%E7%B3%BB%E7%BB%9F/F2D3025E-A8F0-4116-8ABC-053BE5116631.png)
##### 寄存器：
每个寄存器可以⽤来存储⼀定的字节（byte）的数据。⽐如：32 位 CPU 中⼤多数寄存器可以存储  4  个字节；64 位 CPU 中⼤多数寄存器可以存储  8  个字节。
寄存器的访问速度⾮常快，⼀般要求在半个 CPU 时钟周期内完成读写。
##### CPU Cache：
CPU Cache ⽤的是⼀种叫  SRAM（Static Random-Access Memory，静态随机存储器）  的芯⽚。 
SRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，⽽⼀旦断电，数据就会丢失了。在 SRAM ⾥⾯，⼀个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不⾼，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度⾮常快。
CPU 的⾼速缓存，通常可以分为 L1、L2、L3 这样的三层⾼速缓存，也称为⼀级缓存、⼆次缓存、三次缓存。
![](%E7%B3%BB%E7%BB%9F/7E00DC3B-EB13-4018-BA22-20AEA51A03A4.png)
##### L1 ⾼速缓存：
L1 ⾼速缓存的访问速度⼏乎和寄存器⼀样快，通常只需要  2~4  个时钟周期，⽽⼤⼩在⼏⼗ KB 到⼏百KB 不等。
每个 CPU 核⼼都有⼀块属于⾃⼰的 L1 ⾼速缓存，指令和数据在 L1 是分开存放的，所以 L1 ⾼速缓存通常分成**指令缓存**和**数据缓存**。
##### L2 ⾼速缓存
L2 ⾼速缓存同样每个 CPU 核⼼都有，但是 L2 ⾼速缓存位置⽐ L1 ⾼速缓存距离 CPU 核⼼更远，它⼤⼩⽐ L1 ⾼速缓存更⼤，CPU 型号不同⼤⼩也就不同，通常⼤⼩在⼏百 KB 到⼏ MB 不等，访问速度则更慢，速度在10~20  个时钟周期。
##### L3 ⾼速缓存
L3 ⾼速缓存通常是多个 CPU 核⼼共⽤的，位置⽐ L2 ⾼速缓存距离 CPU 核⼼  更远，⼤⼩也会更⼤些，通常⼤⼩在⼏ MB 到⼏⼗ MB 不等，具体值根据 CPU 型号⽽定。访问速度相对也⽐较慢⼀些，访问速度在 20~60 个时钟周期。
##### 内存
内存⽤的芯⽚和 CPU Cache 有所不同，它使⽤的是⼀种叫作  DRAM （Dynamic Random Access Memory，动态随机存取存储器）  的芯⽚。
相⽐ SRAM，DRAM 的密度更⾼，功耗更低，有更⼤的容量，⽽且造价⽐ SRAM 芯⽚便宜很多。 DRAM 存储⼀个 bit 数据，只需要⼀个晶体管和⼀个电容就能存储，但是因为数据会被存储在电容⾥，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。DRAM 的数据访问电路和刷新电路都⽐ SRAM 更复杂，所以访问的速度会更慢，内存速度⼤概在200~300  个  时钟周期之间。
##### SSD 硬盘
SSD（Solid-state disk）  就是我们常说的固体硬盘，结构和内存类似，但是它相⽐内存的优点是断电后数据还是存在的，⽽内存、寄存器、⾼速缓存断电后数据都会丢失。内存的读写速度⽐ SSD ⼤概快10~1000  倍。
##### HDD硬盘
机械硬盘（Hard Disk Drive, HDD），它是通过物理读写的⽅式来访问数据的，因此它访问速度是⾮常慢的，它的速度⽐内存慢 10W  倍左右。

### 如何写出让 CPU 跑得更快的代码
需要写出缓存命中率⾼的代码。
CPU L1 Cache 分为数据缓存和指令缓存，因⽽需要分别提⾼它们的缓存命中率：
* 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为 CPU Cache 是根据 CPU Cache Line 批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升； 
* 对于指令缓存，有规律的条件分⽀语句能够让 CPU 的分⽀预测器发作⽤，进⼀步提⾼执⾏的效率；
另外，对于多核 CPU 系统，线程可能在不同 CPU 核⼼来回切换，这样各个核⼼的缓存命中率就会受到影响，于是要想提⾼进程的缓存命中率，可以考虑把线程绑定 CPU 到某⼀个 CPU 核⼼。

### CPU缓存一致性
#### CPU Cache的数据写入
* 写直达：
把数据同时写⼊内存和 Cache 中，这种⽅法称为写直达 （Write Through）。
![](%E7%B3%BB%E7%BB%9F/C2252F5B-3AC7-4D2F-B3EA-BDC624D6ED57.png)
写直达法很直观，也很简单，但是问题明显，⽆论数据在不在 Cache ⾥⾯，每次写操作都会写回到内存，这样写操作将会花费⼤量的时间，⽆疑性能会受到很⼤的影响。
* 写回：
在写回机制中，当发⽣写操作时，新的数据仅仅被写⼊ Cache Block ⾥，只有当修改过的 Cache Block 「被替换」时才需要写到内存中，减少了数据写回内存的频率，这样便可以提⾼系统的性能。
![](%E7%B3%BB%E7%BB%9F/62EF955F-E54A-4084-8234-F253B82F431A.png)
![](%E7%B3%BB%E7%BB%9F/52E2F7EE-987B-4E1A-920D-8AFB260D8E92.png)
写回这个⽅法，在把数据写⼊到 Cache 的时候，只有在缓存不命中，同时数据对应的 Cache 中的 Cache Block 为脏标记的情况下，才会将数据写到内存中，⽽在缓存命中的情况下，则在写⼊后 Cache后，只需把该数据对应的 Cache Block 标记为脏即可，⽽不⽤写到内存⾥。这样的好处是，如果我们⼤量的操作都能够命中缓存，那么⼤部分时间⾥ CPU 都不需要读写内存，⾃然性能相⽐写直达会⾼很多。
#### 缓存⼀致性问题
现在 CPU 都是多核的，由于 L1/L2 Cache 是多个核⼼各⾃独有的，那么会带来多核⼼的缓存⼀致性 （Cache Coherence）  的问题，如果不能保证缓存⼀致性的问题，就可能造成结果错误。
要想实现缓存⼀致性，关键是要满⾜ 2 点：
* 第⼀点是写传播（Wreite Propagation），也就是当某个 CPU 核⼼发⽣写⼊操作时，需要把该事件⼴播通知给其他核⼼； 
* 第⼆点是事物的串⾏化（Transaction Serialization），这个很重要，只有保证了这个，才能保障我们的数据是真正⼀致的，我们的程序在各个不同的核⼼上运⾏的结果也是⼀致的；
##### 总线嗅探
写传播的原则就是当某个 CPU 核⼼更新了 Cache 中的数据，要把该事件⼴播通知到其他核⼼。最常⻅实现的⽅式是总线嗅探（Bus Snooping）。
总线嗅探的工作机制：通过总线把事件⼴播通知给其他所有的核⼼，然后每个 CPU 核⼼都会监听总线上的⼴播事件，并检查是否有相同的数据在⾃⼰的 L1 Cache ⾥⾯，如果 B 号 CPU 核⼼的 L1 Cache 中有该数据，那么也需要把该数据更新到⾃⼰的 L1 Cache。
可以发现，总线嗅探⽅法很简单， CPU 需要每时每刻监听总线上的⼀切活动，但是不管别的核⼼的Cache 是否缓存相同的数据，都需要发出⼀个⼴播事件，这⽆疑会加重总线的负载。
另外，总线嗅探只是保证了某个 CPU 核⼼的 Cache 更新数据这个事件能被其他 CPU 核⼼知道，但是并不能保证事务串形化。
##### MESI协议
MESI 协议其实是 4 个状态单词的开头字⺟缩写，分别是：
* Modified，已修改 
* Exclusive，独占 
* Shared，共享 
* Invalidated，已失效
这四个状态来标记 Cache Line 四个不同的状态：
**「已修改」**状态就是我们前⾯提到的脏标记，代表该 Cache Block 上的数据已经被更新过，但是还没有写到内存⾥。⽽「已失效」状态，表示的是这个 Cache Block ⾥的数据已经失效了，不可以读取该状态的数据。
**「独占」**和**「共享」**状态都代表 Cache Block ⾥的数据是⼲净的，也就是说，这个时候 Cache Block ⾥的数据和内存⾥⾯的数据是⼀致性的。
**「独占」**和**「共享」**的差别在于，独占状态的时候，数据只存储在⼀个 CPU 核⼼的 Cache ⾥，⽽其他CPU 核⼼的 Cache 没有该数据。这个时候，如果要向独占的 Cache 写数据，就可以直接⾃由地写⼊，⽽不需要通知其他 CPU 核⼼，因为只有你这有这个数据，就不存在缓存⼀致性的问题了，于是就可以随便操作该数据。
另外，在**「独占」**状态下的数据，如果有其他核⼼从内存读取了相同的数据到各⾃的 Cache ，那么这个时候，独占状态下的数据就会变成共享状态。
那么，**「共享」**状态代表着相同的数据在多个 CPU 核⼼的 Cache ⾥都有，所以当我们要更新 Cache ⾥⾯的数据的时候，不能直接修改，⽽是要先向所有的其他 CPU 核⼼⼴播⼀个请求，要求先把其他核⼼的Cache 中对应的 Cache Line 标记为「⽆效」状态，然后再更新当前 Cache ⾥⾯的数据。
所以，可以发现当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送⼴播给其他 CPU 核⼼，这在⼀定程度上减少了总线带宽压⼒。
MESI 协议的状态图：
![](%E7%B3%BB%E7%BB%9F/EA0447EE-3F8B-41D2-83A7-74E4654D657E.png)
MESI 协议的四种状态之间的流转过程：
![](%E7%B3%BB%E7%BB%9F/8B7ED94D-602F-4DBF-BCAA-A842AC7C94BB.png)
![](%E7%B3%BB%E7%BB%9F/41E1AAC9-D091-45DC-BC25-1C7294EAAA69.png)
![](%E7%B3%BB%E7%BB%9F/E849488E-A94B-4F4B-BB9B-27897C24826A.png)

### Cache 伪共享
这种因为多个线程同时读写同⼀个 Cache Line 的不同变量时，⽽导致 CPU Cache 失效的现象称为伪共享（False Sharing）。
##### 避免伪共享的⽅法：
对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同⼀个 Cache Line中，否则就会出现为伪共享的问题。
避免  Cache 伪共享实际上是⽤空间换时间的思想，浪费⼀部分 Cache 空间，从⽽换来性能的提升。
在 Linux 内核中存在     __cacheline_aligned_in_smp  宏定义，是⽤于解决伪共享的问题。
![](%E7%B3%BB%E7%BB%9F/4BBA4B99-B617-4BFF-AFE1-695A1A611DA7.png)
在应⽤层⾯的规避⽅案，有⼀个 Java 并发框架 Disruptor 使⽤「字节填充 + 继承」的⽅式，来避免伪共享的问题。

### CPU调度
在 Linux 内核中，进程和线程都是⽤ tark_struct 结构体表示的，Linux 内核⾥的调度器调度的对象就是tark_struct ，我们把这个数据结构统称为任务。
在 Linux 系统中，根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越⼩，优先级越⾼：
* 实时任务，对系统的响应时间要求很⾼，也就是要尽可能快的执⾏实时任务，优先级在0~99  范围内的就算实时任务；
* 普通任务，响应时间没有很⾼的要求，优先级在 100~139  范围内都是普通任务级别；
![](%E7%B3%BB%E7%BB%9F/E61CFBA0-BFB7-43F6-A12C-13AAC4D87FFC.png)
##### 调度类
由于任务有优先级之分，Linux 系统为了保障⾼优先级的任务能够尽可能早的被执⾏，于是分为了这⼏种调度类，如下图：
![](%E7%B3%BB%E7%BB%9F/892D72B3-1215-4976-B9FD-C8AAAC567313.png)
Deadline 和 Realtime 这两个调度类，都是应⽤于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作⽤如下：
* SCHED_DEADLINE：是按照 deadline 进⾏调度的，距离当前时间点最近的 deadline 的任务会被优先调度；
* SCHED_FIFO：对于相同优先级的任务，按先来先服务的原则，但是优先级更⾼的任务，可以抢占低优先级的任务，也就是优先级⾼的可以「插队」；
* SCHED_RR：对于相同优先级的任务，轮流着运⾏，每个任务都有⼀定的时间⽚，当⽤完时间⽚的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是⾼优先级的任务依然可以抢占低优先级的任务；

Fair 调度类是应⽤于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：
* SCHED_NORMAL：普通任务使⽤的调度策略；
* SCHED_BATCH：后台任务的调度策略，不和终端进⾏交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。

###### 完全公平调度
在 Linux ⾥⾯，实现了⼀个基于 CFS 的调度算法，也就是完全公平调度（Completely Fair Scheduling）。
这个算法的理念是想让分配给每个任务的 CPU 时间是⼀样，于是它为每个任务安排⼀个虚拟运⾏时间vruntime，如果⼀个任务在运⾏，其运⾏的越久，该任务的 vruntime ⾃然就会越⼤，⽽没有被运⾏的任务，vruntime 是不会变化的。那么，在 CFS 算法调度的时候，会优先选择 vruntime 少的任务，以保证每个任务的公平性。
###### 调整优先级
如果想让某个普通任务有更多的执⾏时间，可以调整任务的 nice 值，从⽽让优先级⾼⼀些的任务执⾏更多时间。nice 的值能设置的范围是 -20～19 ， 值越低，表明优先级越⾼，因此 -20 是最⾼优先级，19 则是最低优先级，默认优先级是 0。
priority(new) = priority(old) + nice

### 软中断
#### 中断
在计算机中，中断是系统⽤来响应硬件设备请求的⼀种机制，操作系统收到硬件的中断请求，会打断正在执⾏的进程，然后调⽤内核中的中断处理程序来响应请求。
中断请求的响应程序，也就是中断处理程序，要尽可能快的执⾏完，这样可以减少对正常进程运⾏调度地影响。
⽽且，中断处理程序在响应中断时，可能还会「临时关闭中断」，这意味着，如果当前中断处理程序没有执⾏完之前，系统中其他的中断请求都⽆法被响应，也就说中断有可能会丢失。
#### 软中断
 Linux 系统为了解决中断处理程序执⾏过⻓和中断丢失的问题，将中断过程分成了两个阶段，分别是 「上半部和下半部分」。
* 上半部⽤来快速处理中断，⼀般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。
* 下半部⽤来延迟处理上半部未完成的⼯作，⼀般以「内核线程」的⽅式运⾏。
中断处理程序的上部分和下半部可以理解为：
* 上半部直接处理硬件请求，也就是硬中断，主要是负责耗时短的⼯作，特点是快速执⾏； 
* 下半部是由内核触发，也就说软中断，主要是负责上半部未完成的⼯作，通常都是耗时⽐较⻓的事情，特点是延迟执⾏；
还有⼀个区别，硬中断（上半部）是会打断 CPU 正在执⾏的任务，然后⽴即执⾏中断处理程序，⽽软中断（下半部）是以内核线程的⽅式执⾏，并且每⼀个 CPU 都对应⼀个软中断内核线程，名字通常为「ksoftirqd/CPU 编号」，⽐如 0 号 CPU 对应的软中断内核线程的名字是     ksoftirqd/0
不过，软中断不只是包括硬件设备中断处理程序的下半部，⼀些内核⾃定义事件也属于软中断，⽐如内核调度等、RCU 锁（内核⾥常⽤的⼀种锁）等。
##### 如何定位软中断 CPU 使⽤率过⾼的问题？
Linux 中的软中断包括⽹络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使⽤ watch -d cat /proc/softirqs 命令。
每⼀个 CPU 都有各⾃的软中断内核线程，我们还可以⽤ ps 命令来查看内核线程，⼀般名字在中括号⾥⾯到，都认为是内核线程。
如果在 top 命令发现，CPU 在软中断上的使⽤率⽐较⾼，⽽且 CPU 使⽤率最⾼的进程也是软中断ksoftirqd 的时候，这种⼀般可以认为系统的开销被软中断占据了。
这时我们就可以分析是哪种软中断类型导致的，⼀般来说都是因为⽹络接收软中断导致的，如果是的话，可以⽤ sar 命令查看是哪个⽹卡的有⼤量的⽹络包接收，再⽤ tcpdump 抓⽹络包，做进⼀步分析该⽹络包的源头是不是⾮法地址，如果是就需要考虑防⽕墙增加规则，如果不是，则考虑硬件升级等。

### 浮点数
![](%E7%B3%BB%E7%BB%9F/3A4CA2DE-5BBC-4D29-A407-2C84C366BE5C.png)
![](%E7%B3%BB%E7%BB%9F/A2A24E5B-6FB1-4295-BC73-755739B0BBAE.png)
可以看到：
* double 的尾数部分是 52 位，float 的尾数部分是 23 位，由于同时都带有⼀个固定隐含位（⼆进制浮点数的⼩数点左侧只能有 1 位，并且还只能是 1），所以 double 有 53 个⼆进制有效位，float 有 24 个⼆进制有效位，所以所以它们的精度在⼗ 进制中分别是  log10(2^53)  约等于 15.95  和     log10(2^24)  约等于  7.22   位，因此 double 的有效数字是 15~16  位，float 的有效数字是 7~8  位，这些是有效位是包含整数部分和⼩数部分； 
* double 的指数部分是 11 位，⽽ float 的指数位是 8 位，意味着 double 相⽐ float 能表示更⼤的数值范围；
从 float 的⼆进制浮点数转换成⼗进制时，要考虑到这个隐含的 1，转换公式如下：
![](%E7%B3%BB%E7%BB%9F/D54F7A92-F53D-46C3-B149-6D300A8B4A36.png)
###### 转换举例：
* 十进制小数转二进制：
以 8.625  转⼆进制为例：
![](%E7%B3%BB%E7%BB%9F/B4C7C1C6-32D5-470B-9FA5-DB63003764E7.png)
* ⼆进制⼩数转换成⼆进制浮点数：
以10.625 转 float 为例：
![](%E7%B3%BB%E7%BB%9F/A6FCE774-A411-48AE-87C8-95E1E5AFD66E.png)
*  float 数据转换成⼗进制：
![](%E7%B3%BB%E7%BB%9F/B000828C-27A6-4316-87BC-F30F31E5EB1B.png)


## 操作系统结构
### 内核
#### 内核的能力：
现代操作系统，内核⼀般会提供 4 个基本能⼒：
* 管理进程、线程，决定哪个进程、线程使⽤ CPU，也就是进程调度的能⼒； 
* 管理内存，决定内存的分配和回收，也就是内存管理的能⼒；
* 管理硬件设备，为进程与硬件设备之间提供通信能⼒，也就是硬件通信能⼒；
* 提供系统调⽤，如果应⽤程序要运⾏更⾼权限运⾏的服务，那么就需要有系统调⽤，它是⽤户程序与操作系统之间的接⼝。
#### 内核的⼯作：
内核具有很⾼的权限，可以控制 cpu、内存、硬盘等硬件，⽽应⽤程序具有的权限很⼩，因此⼤多数操作系统，把内存分成了两个区域：
* 内核空间，这个内存空间只有内核程序可以访问； 
* ⽤户空间，这个内存空间专⻔给应⽤程序使⽤；
![](%E7%B3%BB%E7%BB%9F/23E2A4AB-1DD2-43B0-B475-F170BE187D01.png)
内核程序执⾏在内核态，⽤户程序执⾏在⽤户态。当应⽤程序使⽤系统调⽤时，会产⽣⼀个中断。发⽣中断后， CPU 会中断当前在执⾏的⽤户程序，转⽽跳转到中断处理程序，也就是开始执⾏内核程序。内核处理完后，主动触发中断，把 CPU 执⾏权限交回给⽤户程序，回到⽤户态继续⼯作。
### Linux的设计
Linux 内核设计的理念主要有这⼏个点：
* MutiTask，多任务
多任务意味着可以有多个任务同时执⾏，这⾥的「同时」可以是并发或并⾏。
* SMP，对称多处理
代表着每个 CPU 的地位是相等的，对资源的使⽤权限也是相同的，多个 CPU 
共享同⼀个内存，每个 CPU 都可以访问完整的内存和硬件资源。
这个特点决定了 Linux 操作系统不会有某个 CPU 单独服务应⽤程序或内核程序，⽽是每个程序都可以被分配到任意⼀个 CPU 上被执⾏。
* ELF，可执⾏⽂件链接格式 
它是 Linux 操作系统中可执⾏⽂件的存储格式：
![](%E7%B3%BB%E7%BB%9F/07BCA1C9-2080-4971-A64E-5FE56F596A23.png)
ELF ⽂件有两种索引：Program header table 中记录了「运⾏时」所需的段，⽽ Section header table 记录了⼆进制⽂件中各个「段的⾸地址」。
* Monolithic Kernel，宏内核
意味着 Linux 的内核是⼀个完整的可执⾏程序，且拥有最⾼的权限。
宏内核的特征是系统内核的所有模块，⽐如进程调度、内存管理、⽂件系统、设备驱动等，都运⾏在内核态。
不过，Linux 也实现了动态加载内核模块的功能，例如⼤部分设备驱动是以可加载模块的形式存在的，与内核其他模块解藕，让驱动开发和驱动加载更为⽅便、灵活。
> 与宏内核相反的是微内核：微内核架构的内核只保留最基本的能⼒，⽐如进程调度、虚拟机内存、中断等，把⼀些应⽤放到了⽤户空间，⽐如驱动程序、⽂件系统等。这样服务与服务之间是隔离的，单个服务出现故障或者完全攻击，也不会导致整个操作系统挂掉，提⾼了操作系统的稳定性和可靠性。  
> 微内核内核功能少，可移植性⾼，相⽐宏内核有⼀点不好的地⽅在于，由于驱动程序不在内核中，⽽且驱动程序⼀般会频繁调⽤底层能⼒的，于是驱动和硬件设备交互就需要频繁切换到内核态，这样会带来性能损耗。  
> 还有⼀种内核叫混合类型内核，它的架构有点像微内核，内核⾥⾯会有⼀个最⼩版本的内核，然后其他模块会在这个基础上搭建，然后实现的时候会跟宏内核类似，也就是把整个内核做成⼀个完整的程序，⼤部分服务都在内核中，这就像是宏内核的⽅式包裹着⼀个微内核。  
### Windows 的设计
当今 Windows 7、Windows 10 使⽤的内核叫 Windows NT，NT 全称叫 New Technology。 
下图是 Windows NT 的结构图⽚：
![](%E7%B3%BB%E7%BB%9F/84CE9513-72EF-41A9-85D2-608FF7F3643D.png)
Windows 和 Linux ⼀样，同样⽀持 MutiTask 和 SMP，但不同的是，Window 的内核设计是**混合型内核**， 
在上图你可以看到内核中有⼀个 MicroKernel 模块，这个就是最⼩版本的内核，⽽整个内核实现是⼀个完整的程序，含有⾮常多模块。
Windows 的可执⾏⽂件格式叫 PE，称为可移植执⾏⽂件，扩展名通常是   .exe 、   .dll 、   .sys 等。PE 的结构如下图：
![](%E7%B3%BB%E7%BB%9F/2E732B5F-06CD-4A3A-9D39-E09846528D1B.png)



## 进程与线程
### 进程
#### 进程状态变迁：
![](%E7%B3%BB%E7%BB%9F/CC7448DB-3315-48A0-A3C3-3E11E4993AEC.png)
挂起状态：描述进程没有占⽤实际的物理内存空间的情况。 这跟阻塞状态是不⼀样，阻塞状态是等待某个事件的返回。
挂起状态可以分为两种：
* 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
* 就绪挂起状态：进程在外存（硬盘），但只要进⼊内存，即刻⽴刻运⾏；
导致进程挂起的原因不只是因为进程所使⽤的内存空间不在物理内存，还包括如下情况：
* 通过 sleep 让进程间歇性挂起，其⼯作原理是设置⼀个定时器，到期后唤醒进程。 
* ⽤户希望挂起⼀个程序的执⾏，⽐如在 Linux 中⽤  Ctrl+Z  挂起进程；
#### 进程的控制结构
PCB ：process control block 进程控制块，操作系统用PCB数据结构来描述进程的。
PCB 是进程存在的唯⼀标识；
PCB 具体包含信息：
1）进程描述信息：
* 进程标识符：标识各个进程，每个进程都有⼀个并且唯⼀的标识符； 
* ⽤户标识符：进程归属的⽤户，⽤户标识符主要为共享和保护服务；
2）进程控制和管理信息：
* 进程当前状态，如 new、ready、running、waiting 或 blocked 等； 
* 进程优先级：进程抢占 CPU 时的优先级；
3）资源分配清单：
* 有关内存地址空间或虚拟地址空间的信息，所打开⽂件的列表和所使⽤的 I/O 设备信息。
4）CPU 相关信息：
* CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执⾏时，能从断点处继续执⾏。
#### PCB组织形式
* 链表形式：
![](%E7%B3%BB%E7%BB%9F/D807CE62-D68B-491E-B3C5-DFE857DA17DD.png)
* 索引形式：
⼯作原理：将同⼀状态的进程组织在⼀个索引表中，索引表项指向相应的 PCB，不同状态对应不同的索引表。
⼀般会选择链表，因为可能⾯临进程创建，销毁等调度导致进程状态发⽣变化，所以链表能够更加灵活的插⼊和删除。
#### 进程的上下⽂切换
进程是由内核管理和调度的，所以进程的切换只能发⽣在内核态。
进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。
##### 发⽣进程上下⽂切换的场景：
* 为了保证所有进程可以得到公平调度，CPU 时间被划分为⼀段段的时间⽚，这些时间⽚再被轮流分配给各个进程。这样，当某个进程的时间⽚耗尽了，进程就从运⾏状态变为就绪状态，系统从就绪队列选择另外⼀个进程运⾏；
* 进程在系统资源不⾜（⽐如内存不⾜）时，要等到资源满⾜后才可以运⾏，这个时候进程也会被挂起，并由系统调度其他进程运⾏；
* 当进程通过睡眠函数 sleep 这样的⽅法将⾃⼰主动挂起时，⾃然也会重新调度；
* 当有优先级更⾼的进程运⾏时，为了保证⾼优先级进程的运⾏，当前进程会被挂起，由⾼优先级进程来运⾏；
* 发⽣硬件中断时，CPU 上的进程会被中断挂起，转⽽执⾏内核中的中断服务程序；

### 线程
线程是进程当中的⼀条执⾏流程。
同⼀个进程内多个线程之间可以共享代码段、数据段、打开的⽂件等资源，但每个线程各⾃都有⼀套独⽴的寄存器和栈，这样可以确保线程的控制流是相对独⽴的。
##### 线程的优缺点
1）线程的优点：
* ⼀个进程中可以同时存在多个线程； 
* 各个线程之间可以并发执⾏；
* 各个线程之间可以共享地址空间和⽂件等资源； 
2）线程的缺点：
* 当进程中的⼀个线程崩溃时，会导致其所属进程的所有线程崩溃。
##### 线程与进程的⽐较
1）线程与进程的⽐较如下：
* 进程是资源（包括内存、打开的⽂件等）分配的单位，线程是 CPU 调度的单位； 
* 进程拥有⼀个完整的资源平台，⽽线程只独享必不可少的资源，如寄存器和栈； 
* 线程同样具有就绪、阻塞、执⾏三种基本状态，同样具有状态之间的转换关系； 
* 线程能减少并发执⾏的时间和空间开销；
2）线程相⽐进程能减少开销，体现在：
* 线程的创建时间⽐进程快，因为进程在创建的过程中，还需要资源管理信息，⽐如内存管理信息、⽂件管理信息，⽽线程在创建的过程中，不会涉及这些资源管理信息，⽽是共享它们；
* 线程的终⽌时间⽐进程快，因为线程释放的资源相⽐进程少很多；
* 同⼀个进程内的线程切换⽐进程切换快，因为线程具有相同的地址空间（虚拟内存共享），这意味着同⼀个进程的线程都具有同⼀个⻚表，那么在切换的时候不需要切换⻚表。⽽对于进程之间的切换，切换的时候要把⻚表给切换掉，⽽⻚表的切换过程开销是⽐较⼤的；
* 由于同⼀进程的各线程间共享内存和⽂件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更⾼了；
##### 线程上下⽂切换的
这还得看线程是不是属于同⼀个进程：
当两个线程不是属于同⼀个进程，则切换的过程就跟进程上下⽂切换⼀样；
当两个线程是属于同⼀个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据；
所以，线程的上下⽂切换相⽐进程，开销要⼩很多。
##### 线程的实现
主要有三种线程的实现⽅式：
* ⽤户线程（User Thread）：在⽤户空间实现的线程，不是由内核管理的线程，是由⽤户态的线程库来完成线程的管理；
* 内核线程（Kernel Thread）：在内核中实现的线程，是由内核管理的线程； 
* 轻量级进程（LightWeight Process）：在内核中来⽀持⽤户线程；
##### ⽤户线程：
⽤户线程是基于⽤户态的线程管理库来实现的，那么线程控制块（Thread Control Block, TCB）  也是在库⾥⾯来实现的，对于操作系统⽽⾔是看不到这个 TCB 的，它只能看到整个进程的 PCB。
所以，⽤户线程的整个线程管理和调度，操作系统是不直接参与的，⽽是由⽤户级线程库函数来完成线程的管理，包括线程的创建、终⽌、同步和调度等。
###### ⽤户级线程的模型：
多对⼀的关系，即多个⽤户线程对应同⼀个内核线程：
![](%E7%B3%BB%E7%BB%9F/9BE2CEAF-93CC-4E48-8603-25D07457ECBB.png)
###### ⽤户线程的优点：
    * 每个进程都需要有它私有的线程控制块（TCB）列表，⽤来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由⽤户级线程库函数来维护，可⽤于不⽀持线程技术的操作系统；
    * ⽤户线程的切换也是由线程库函数来完成的，⽆需⽤户态与内核态的切换，所以速度特别快； 
###### ⽤户线程的缺点：
    * 由于操作系统不参与线程的调度，如果⼀个线程发起了系统调⽤⽽阻塞，那进程所包含的⽤户线程都不能执⾏了。
    * 当⼀个线程开始运⾏后，除⾮它主动地交出 CPU 的使⽤权，否则它所在的进程当中的其他线程⽆法运⾏，因为⽤户态的线程没法打断当前运⾏中的线程，它没有这个特权，只有操作系统才有，但是⽤户线程不是由操作系统管理的。
    * 由于时间⽚分配给进程，故与其他进程⽐，在多线程执⾏时，每个线程得到的时间⽚较少，执⾏会⽐较慢。
##### 内核线程：
内核线程是由操作系统管理的，线程对应的 TCB ⾃然是放在操作系统⾥的，这样线程的创建、终⽌和管理都是由操作系统负责。
###### 内核线程的模型：
⼀对⼀的关系，即⼀个⽤户线程对应⼀个内核线程：
![](%E7%B3%BB%E7%BB%9F/F9F1593D-FB73-4BF5-BD23-A39738ED58C4.png)
###### 内核线程的优点：
* 在⼀个进程当中，如果某个内核线程发起系统调⽤⽽被阻塞，并不会影响其他内核线程的运⾏； 
* 分配给线程，多线程的进程获得更多的 CPU 运⾏时间；
###### 内核线程的缺点：
* 在⽀持内核线程的操作系统中，由内核来维护进程和线程的上下⽂信息，如 PCB 和 TCB； 
* 线程的创建、终⽌和切换都是通过系统调⽤的⽅式来进⾏，因此对于系统来说，系统开销⽐较⼤；
##### 轻量级进程（Light-weight process，LWP）
轻量级进程 LWP 是内核⽀持的⽤户线程，⼀个进程可有⼀个或多个 LWP，每 
个 LWP 是跟内核线程⼀对⼀映射的，也就是 LWP 都是由⼀个内核线程⽀持。
另外，LWP 只能由内核管理并像普通进程⼀样被调度，Linux 内核是⽀持 LWP 的典型例⼦。
在⼤多数系统中，LWP与普通进程的区别也在于它只有⼀个最⼩的执⾏上下⽂和调度程序所需的统计信息。
###### LWP模型
在 LWP 之上也是可以使⽤⽤户线程的，那么 LWP 与⽤户线程的对应关系就有三种：
1 : 1 ，即⼀个 LWP 对应 ⼀个⽤户线程； 
N : 1 ，即⼀个 LWP 对应多个⽤户线程； 
M : N ，即多个 LMP 对应多个⽤户线程；
下图LWP 模型：
![](%E7%B3%BB%E7%BB%9F/520EB7B0-AE2E-4171-9489-D9CC370F8BB7.png)
* 1 : 1 模式
⼀个线程对应到⼀个 LWP 再对应到⼀个内核线程，如上图的进程 4，属于此模型。
优点：实现并⾏，当⼀个 LWP 阻塞，不会影响其他 LWP；
缺点：每⼀个⽤户线程，就产⽣⼀个内核线程，创建线程的开销较⼤。
* N : 1 模式
多个⽤户线程对应⼀个 LWP 再对应⼀个内核线程，如上图的进程 2，线程管理是在⽤户空间完成的，此模式中⽤户的线程对操作系统不可⻅。
优点：⽤户线程要开⼏个都没问题，且上下⽂切换发⽣⽤户空间，切换的效率较⾼；
缺点：⼀个⽤户线程如果阻塞了，则整个进程都将会阻塞，另外在多核 CPU 中，是没办法充分利⽤CPU 的。
* M : N 模式
根据前⾯的两个模型混搭⼀起，就形成  M:N  模型，该模型提供了两级控制，⾸先多个⽤户线程对应到多 个 LWP，LWP 再⼀⼀对应到内核线程，如上图的进程 3。
优点：综合了前两种优点，⼤部分的线程上下⽂发⽣在⽤户空间，且多个线程⼜可以充分利⽤多核CPU 的资源。
* 组合模式
如上图的进程 5，此进程结合 1:1  模型和 M:N  模型。开发⼈员可以针对不同的应⽤特点调节内核线程的数⽬来达到物理并⾏性和逻辑并⾏性的最佳⽅案。

### 进程间调度
##### 调度时机
在进程的⽣命周期中，当进程从⼀个运⾏状态到另外⼀状态变化的时候，其实会触发⼀次调度。 
⽐如，以下状态的变化都会触发操作系统的调度：
* 从就绪态 -> 运⾏态：当进程被创建时，会进⼊到就绪队列，操作系统会从就绪队列选择⼀个进程运⾏；
* 从运⾏态 -> 阻塞态：当进程发⽣ I/O 事件⽽阻塞时，操作系统必须另外⼀个进程运⾏；
* 从运⾏态 -> 结束态：当进程退出结束后，操作系统得从就绪队列选择另外⼀个进程运⾏；
##### 调度原则
* 原则⼀：如果运⾏的程序，发⽣了 I/O 事件的请求，那 CPU 使⽤率必然会很低，因为此时进程在阻塞等待硬盘的数据返回。这样的过程，势必会造成 CPU 突然的空闲。**所以，为了提⾼ CPU 利⽤率，在这种发送I/O 事件致使 CPU 空闲的情况下，调度程序需要从就绪队列中选择⼀个进程来运⾏。**
* 原则⼆：有的程序执⾏某个任务花费的时间会⽐较⻓，如果这个程序⼀直占⽤着 CPU，会造成系统吞吐量（CPU 在单位时间内完成的进程数量）的降低。**所以，要提⾼系统的吞吐率，调度程序要权衡⻓任务和短任务进程的运⾏完成数量。**
* 原则三：从进程开始到结束的过程中，实际上是包含两个时间，分别是进程运⾏时间和进程等待时间，这两个时间总和就称为周转时间。进程的周转时间越⼩越好，**如果进程的等待时间很⻓⽽运⾏时间很短，那周转时间就很⻓，这不是我们所期望的，调度程序应该避免这种情况发⽣。**
* 原则四：处于就绪队列的进程，也不能等太久，当然希望这个等待的时间越短越好，这样可以使得进程更快的在 CPU 中执⾏。**所以，就绪队列中进程的等待时间也是调度程序所需要考虑的原则。**
* 原则五：对于⿏标、键盘这种交互式⽐较强的应⽤，我们当然希望它的响应时间越快越好，否则就会影响⽤户体验了。**所以，对于交互式⽐较强的应⽤，响应时间也是调度程序需要考虑的原则。**
针对上⾯的五种调度原则，总结成如下：
1）CPU 利⽤率：调度程序应确保 CPU 是始终匆忙的状态，这可提⾼ CPU 的利⽤率；
2）系统吞吐量：吞吐量表示的是单位时间内 CPU 完成进程的数量，⻓作业的进程会占⽤较⻓的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
3）周转时间：周转时间是进程运⾏和阻塞时间总和，⼀个进程的周转时间越⼩越好；
4）等待时间：这个等待时间不是阻塞状态的时间，⽽是进程处于就绪队列的时间，等待的时间越⻓，⽤户越不满意；
5）响应时间：⽤户提交请求到系统第⼀次产⽣响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。
##### 调度算法
单核 CPU 系统中常⻅的调度算法，如下：
* 先来先服务（First Come First Seved, FCFS）：
每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。
FCFS 对⻓作业有利，适⽤于 CPU 繁忙型作业的系统，⽽不适⽤于 I/O 繁忙型作业的系统。
* 最短作业优先（Shortest Job First, SJF）：
优先选择运⾏时间最短的进程来运⾏，这有助于提⾼系统的吞吐量。
* ⾼响应⽐优先（Highest Response Ratio Next, HRRN）：
每次进⾏进程调度时，先计算「响应⽐优先级」，然后把「响应⽐优先级」最⾼的进程投⼊运⾏。
「响应⽐优先级」的计算公式：
![](%E7%B3%BB%E7%BB%9F/41374C1C-6A08-437A-8618-3BFB5287BA43.png)
如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应⽐」就越⾼，这样短作业的进程容易被选中运⾏；
如果两个进程「要求的服务时间」相同时，「等待时间」越⻓，「响应⽐」就越⾼，这就兼顾到了⻓作业进程，因为进程的响应⽐可以随时间等待的增加⽽提⾼，当其等待时间⾜够⻓时，其响应⽐便可以升到很⾼，从⽽获得运⾏的机会；
* 时间⽚轮转（Round Robin, RR）：
每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。
如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外⼀个进程；
如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换；
⼀般来说，时间⽚设为     20ms~50ms  通常是⼀个⽐较合理的折中值。
* 最⾼优先级（Highest Priority First，HPF）调度算法：
从就绪队列中选择最⾼优先级的进程进⾏运⾏。
进程的优先级可以分为，静态优先级和动态优先级：
        静态优先级：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化； 
        动态优先级：根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时间的推移增加等待进程的优先级。
该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：
         ⾮抢占式：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。 
         抢占式：当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。
* 多级反馈队列（Multilevel Feedback Queue）调度算法：
「多级」表示有多个队列，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短。
「反馈」表示如果有新的进程加⼊优先级⾼的队列时，⽴刻停⽌当前正在运⾏的进程，转⽽去运⾏优先级⾼的队列；
![](%E7%B3%BB%E7%BB%9F/7D91D1AF-24D3-4532-9208-13B12C7C3478.png)
可以发现，对于短作业可能可以在第⼀级队列很快被处理完。对于⻓作业，如果在第⼀级队列处理不完，可以移⼊下次队列等待被执⾏，虽然等待的时间变⻓了，但是运⾏时间也变更⻓了，所以该算法很好的兼顾了⻓短作业，同时有较好的响应时间。

### 进程间通信
#### 管道
* 匿名管道：没有名字标识，匿名管道是特殊⽂件只存在于内存，没有存在于⽂件系统中。
匿名管道的创建，需要通过下⾯这个系统调⽤：
![](%E7%B3%BB%E7%BB%9F/18EB01E6-8D0C-4370-B783-12F5D94B20BA.png)
它的通信范围是存在⽗⼦关系的进程。因为管道没有实体，也就是没有管道⽂件，只能通过 fork 来复制⽗进程 fd ⽂件描述符，来达到通信的⽬的。
shell 命令中的「 | 」竖线就是匿名管道：
![](%E7%B3%BB%E7%BB%9F/9FD9650D-EBA9-4EF0-9748-0A24AABD8539.png)
* 命名管道：也被叫做 FIFO ，因为数据是先进先出的传输⽅式。
它可以在不相关的进程间也能相互通信。因为命名管道提前创建了⼀个类型为管道的设备⽂件，在进程⾥只要使⽤这个设备⽂件，就可以相互通信。
不管是匿名管道还是命名管道，进程写⼊的数据都是缓存在内核中，另⼀个进程读取数据时候⾃然也是从内核中获取，同时通信数据都遵循先进先出原则，不⽀持 lseek 之类的⽂件定位操作。
管道这种通信⽅式效率低，不适合进程间频繁地交换数据。当然，它的好处，⾃然就是简单，同时也我们很容易得知管道⾥的数据已经被另⼀个进程读取了。
#### 消息队列
**消息队列是保存在内核中的消息链表**，在发送数据时，会分成⼀个⼀个独⽴的数据单元，也就是消息体（数据块），消息体是⽤户⾃定义的数据类型，消息的发送⽅和接收⽅要约定好消息体的数据类型，所以每个消息体都是固定⼤⼩的存储块，不像管道是⽆格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。
消息队列⽣命周期随内核，**如果没有释放消息队列或者没有关闭操作系统，消息队列会⼀直存在**，⽽匿名管道的⽣命周期，是随进程的创建⽽建⽴，随进程的结束⽽销毁。
消息队列的缺点：
1）**消息队列不适合⽐较⼤数据的传输**，因为在内核中每个消息体都有⼀个最⼤⻓度的限制，同时所有队列所包含的全部消息体的总⻓度也是有上限。在 Linux 内核中，会有两个宏定义 MSGMAX 和 MSGMNB ，它们以字节为单位，分别定义了⼀条消息的最⼤⻓度和⼀个队列的最⼤⻓度。
2）**消息队列通信过程中，存在⽤户态与内核态之间的数据拷⻉开销**，因为进程写⼊数据到内核中的消息队列时，会发⽣从⽤户态拷⻉数据到内核态的过程，同理另⼀进程读取内核中的消息数据时，会发⽣从内核态拷⻉数据到⽤户态的过程。
#### 共享内存
共享内存的机制，就是拿出⼀块虚拟地址空间来，映射到相同的物理内存中。
![](%E7%B3%BB%E7%BB%9F/8CE7DF8D-067E-4FCD-9374-FC00A477A9CB.png)
#### 信号量
为了防⽌多进程竞争共享资源，⽽造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被⼀个进程访问。正好，信号量就实现了这⼀保护机制。
信号量其实是⼀个整型的计数器，主要⽤于实现进程间的互斥与同步，⽽不是⽤于缓存进程间通信的数据。
信号量表示资源的数量，控制信号量的⽅式有两种原⼦操作：
* ⼀个是  **P 操作**，这个操作会把信号量减去 1，相减后如果信号量 < 0，则表明资源已被占⽤，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使⽤，进程可正常继续执⾏。
* 另⼀个是  ** V 操作**，这个操作会把信号量加上 1，相加后如果信号量 <= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运⾏；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；
P 操作是⽤在进⼊共享资源之前，V 操作是⽤在离开共享资源之后，这两个操作是必须成对出现的。
> 信号初始化为1 ，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有⼀个进程在访问，这就很好的保护了共享内存。  
> 信号初始化为     0 ，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执⾏。  
#### 信号
对于异常情况下的⼯作模式，就需要⽤「信号」的⽅式来通知进程。
信号是进程间通信机制中唯⼀的异步通信机制，因为可以在任何时候发送信号给某⼀进程，⼀旦有信号产⽣，我们就有下⾯这⼏种，⽤户进程对信号的处理⽅式：
* 执⾏默认操作。Linux 对每种信号都规定了默认操作，例如，上⾯列表中的 SIGTERM 信号，就是终⽌进程的意思。
* 捕捉信号。我们可以为信号定义⼀个信号处理函数。当信号发⽣时，我们就执⾏相应的信号处理函数。
* 忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。**有两个信号是应⽤进程⽆法捕捉和忽略的，即 SIGKILL  和 SEGSTOP，** 它们⽤于在任何时候中断或结束某⼀进程。
Linux下的信号：
![](%E7%B3%BB%E7%BB%9F/BEBE8434-7C36-47CF-AB5F-C5AB195E7C37.png)
#### Socket
管道、消息队列、共享内存、信号量和信号都是在同⼀台主机上进⾏进程间通信，Socket 通信不仅可以在同主机上进程间通信，还可以跨⽹络与不同主机上的进程之间通信。
![](%E7%B3%BB%E7%BB%9F/23C3EFFF-2CE8-467C-8ADF-A2BCF5F464C3.png)
###### 针对 TCP 协议通信的 socket 编程模型：
![](%E7%B3%BB%E7%BB%9F/F01E2AD2-C5C3-4897-B759-96B177F833D8.png)
###### 针对 UDP 协议通信的 socket 编程模型：
![](%E7%B3%BB%E7%BB%9F/DE0A60FA-486A-44B8-9F43-57C1487D42B8.png)
###### 针对本地进程间通信的 socket 编程模型：
本地 socket 被⽤于在同⼀台主机上进程间通信的场景：
* 本地 socket 的编程接⼝和 IPv4 、IPv6 套接字编程接⼝是⼀致的，可以⽀持「字节流」和「数据报」 两种协议；
* 本地 socket 的实现效率⼤⼤⾼于 IPv4 和 IPv6 的字节流、数据报 socket 实现； 
对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。 
对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。
本地字节流 socket 和  本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端⼝，⽽是绑定⼀个本地⽂件，这也就是它们之间的最⼤区别。

### 多线程同步
#### 竞争与协作
![](%E7%B3%BB%E7%BB%9F/EB3110FF-DAB2-4D13-8333-4B576E8376EA.png)
多个线程如果竞争共享资源，如果不采取有效的措施，则会造成共享数据的混乱。
###### 互斥的概念
临界区（critical section）是访问共享资源的代码⽚段，⼀定不能给多线程同时执⾏。希望临界区代码是互斥（mutualexclusion）的，也就说保证⼀个线程在临界区执⾏时，其他线程应该被阻⽌进⼊临界区。
###### 同步的概念
所谓同步，就是并发进程/线程在⼀些关键点上可能需要互相等待与互通消息，这种相互制约的等待与互通信息称为进程/线程同步。
#### 互斥与同步的实现和使⽤
在进程/线程并发执⾏的过程中，进程/线程之间存在协作的关系，例如有互斥、同步的关系。
为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和⽅法，主要的⽅法有两种：
* 锁：加锁、解锁操作； 
* 信号量：P、V 操作；
##### 锁
使⽤加锁操作和解锁操作可以解决并发线程/进程的互斥问题。
**根据锁的实现不同，可以分为「忙等待锁」和「⽆忙等待锁」**。
**现代 CPU 体系结构提供了一个特殊原⼦操作指令 —— 测试和置位 （Test-and-Set）指令**。
###### 忙等待锁：
可以运⽤ Test-and-Set 指令来实现「忙等待锁」，代码如下：
![](%E7%B3%BB%E7%BB%9F/F516D69D-44A6-4CA0-81F0-986A52671D62.png)
当获取不到锁时，线程就会⼀直 wile 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**⾃旋锁（spin lock）**。
这是最简单的⼀种锁，⼀直⾃旋，利⽤ CPU 周期，直到锁可⽤。在单处理器上，需要抢占式的调度器（即不断通过时钟中断⼀个线程，运⾏其他线程）。否则，⾃旋锁在单 CPU 上⽆法使⽤，因为⼀个⾃旋的线程永远不会放弃 CPU。
###### ⽆等待锁
顾明思议就是获取不到锁的时候，不⽤⾃旋。当没获取到锁的时候，就把当前线程放⼊到锁的等待队列，然后执⾏调度程序，把 CPU 让给其他线程执⾏。
![](%E7%B3%BB%E7%BB%9F/51B95ECC-2BE8-4425-8F2D-DB2D730B01CE.png)
###### 信号量
信号量表示资源的数量，对应的变量是⼀个整型（ sem ）变量。 
另外，还有两个原⼦操作的系统调⽤函数来控制信号量的，分别是：
* P 操作：将     sem  减     1 ，相减后，如果     sem < 0 ，则进程/线程进⼊阻塞等待，否则继续，表明 P 操作可能会阻塞；
* V 操作：将     sem  加     1 ，相加后，如果     sem <= 0 ，唤醒⼀个等待中的进程/线程，表明 V 操作不会阻塞；
P 操作是⽤在进⼊临界区之前，V 操作是⽤在离开临界区之后，这两个操作是必须成对出现的。
![](%E7%B3%BB%E7%BB%9F/1B604DAF-65A6-4AE3-8CAF-EEFE41F6A225.png)
PV 操作的函数是由操作系统管理和实现的，所以操作系统已经使得执⾏ PV 函数时是具有原⼦性的。
###### ⽣产者-消费者问题
![](%E7%B3%BB%E7%BB%9F/0C8DC46A-E173-486D-AA42-4D910B144FB4.png)
我们需要三个信号量，分别是：
* 互斥信号量     mutex ：⽤于互斥访问缓冲区，初始化值为 1；
* 资源信号量     fullBuffers ：⽤于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0 （表明缓冲区⼀开始为空）；
* 资源信号量     emptyBuffers ：⽤于⽣产者询问缓冲区是否有空位，有空位则⽣成数据，初始化值为 n （缓冲区⼤⼩）；
![](%E7%B3%BB%E7%BB%9F/8BF136BD-386F-4693-A47E-6FDFF3438EC3.png)
###### 哲学家就餐问题
![](%E7%B3%BB%E7%BB%9F/E8ED6D41-2F1E-43DF-9828-9FB169C1CBA5.png)
哲学家就餐的问题描述：
* 5个⽼⼤哥哲学家，闲着没事做，围绕着⼀张圆桌吃⾯；
* 巧就巧在，这个桌⼦只有5⽀叉⼦，每两个哲学家之间放⼀⽀叉⼦； 
* 哲学家围在⼀起先思考，思考中途饿了就会想进餐；
* 奇葩的是，这些哲学家要两⽀叉⼦才愿意吃⾯，也就是需要拿到左右两边的叉⼦才进餐； 
* 吃完后，会把两⽀叉⼦放回原处，继续思考；
那么问题来了，如何保证哲学家们的动作有序进⾏，⽽不会出现有⼈永远拿不到叉⼦呢？
**方案1**：
为了避免哲学家可以同时拿左边的⼑叉，采⽤分⽀结构，根据哲学家的编号的不同，⽽采取不同的动作。让偶数编号的哲学家「先拿左边的叉⼦后拿右边的叉⼦」，奇数编号的哲学家「先拿右边的叉⼦后拿左边的叉⼦」。
![](%E7%B3%BB%E7%BB%9F/8447D47F-AFD0-4F64-9D8C-373D39DE0D28.png)
![](%E7%B3%BB%E7%BB%9F/537DE4A9-8D44-4630-AC3C-1A4EAC57978F.png)
**方案2**：
⽤⼀个数组 state 来记录每⼀位哲学家在进程、思考还是饥 
饿状态（正在试图拿叉⼦）。
那么，⼀个哲学家只有在两个邻居都没有进餐时，才可以进⼊进餐状态。 
第 i 个哲学家的左邻右舍，则由宏     LEFT  和     RIGHT  定义：LEFT : ( i + 5  - 1 ) % 5  ； RIGHT : ( i + 1 ) % 5
![](%E7%B3%BB%E7%BB%9F/04EB509A-2594-4140-AEE9-02D9FD9E5ECF.png)
![](%E7%B3%BB%E7%BB%9F/483ABAAD-AB15-4251-B11A-1D0FA5A24217.png)
![](%E7%B3%BB%E7%BB%9F/3C962C3A-05DF-49F8-A06C-FFCCF99AADC3.png)
![](%E7%B3%BB%E7%BB%9F/5856E5C8-1895-49EE-9431-FB70C633181C.png)
![](%E7%B3%BB%E7%BB%9F/A8CAF5C9-D1B4-424B-AFD6-3832EA064C24.png)
「哲学家进餐问题」对于互斥访问有限的竞争问题（如 I/O 设备）⼀类的建模过程⼗分有⽤。
###### 读者-写者问题
「读者-写者」问题为数据库访问建⽴了⼀个模型。
读者只会读取数据，不会修改数据，⽽写者即可以读也可以修改数据。 
读者-写者的问题描述：
* 「读-读」允许：同⼀时刻，允许多个读者同时读；
* 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写 ；
* 「写-写」互斥：没有其他写者时，写者才能写。
**⽅案⼀**：
使⽤信号量的⽅式来尝试解决：
1）信号量 wMutex ：控制写操作的互斥信号量，初始值为 1 ；
2）读者计数 rCount ：正在进⾏读操作的读者个数，初始化为 0；
3）信号量rCountMutex ：控制对 rCount 读者计数器的互斥修改，初始值为 1；
![](%E7%B3%BB%E7%BB%9F/247B3571-57F1-4433-ADE9-0CFCAC94E888.png)
![](%E7%B3%BB%E7%BB%9F/46A08501-D551-4D7B-8580-347D838E26B3.png)
这种实现，是读者优先的策略，因为只要有读者正在读的状态，后来的读者都可以直接进⼊，如果读者持续不断进⼊，则写者会处于饥饿状态。
**⽅案⼆**：
写者优先策略：
* 只要有写者准备要写⼊，写者应尽快执⾏写操作，后来的读者就必须阻塞； 
* 如果有写者持续不断写⼊，则读者就处于饥饿；
在⽅案⼀的基础上新增如下变量：
* 信号量     rMutex ：控制读者进⼊的互斥信号量，初始值为 1； 
* 信号量     wDataMutex ：控制写者写操作的互斥信号量，初始值为 1； 
* 写者计数     wCount ：记录写者数量，初始值为 0；
* 信号量     wCountMutex ：控制 wCount 互斥修改，初始值为 1；
![](%E7%B3%BB%E7%BB%9F/64A894FB-0FB4-4FD0-823B-BF96997438B4.png)
![](%E7%B3%BB%E7%BB%9F/87F24610-07CE-483F-ADBA-8DB28C65E47A.png)
这⾥ rMutex 的作⽤，开始有多个读者读数据，它们全部进⼊读者队列，此时来了⼀个写者，执⾏了 P(rMutex) 之后，后续的读者由于阻塞在 rMutex 上，都不能再进⼊读者队列，⽽写者到来，则可以全部进⼊写者队列，因此保证了写者优先。
同时，第⼀个写者执⾏了     P(rMutex)  之后，也不能⻢上开始写，必须等到所有进⼊读者队列的读者都执⾏完读操作，通过  V(wDataMutex)  唤醒写者的写操作。
**⽅案三**：
公平策略：
* 优先级相同；
* 写者、读者互斥访问；
* 只能⼀个写者访问临界区；
* 可以有多个读者同时访问临街资源
![](%E7%B3%BB%E7%BB%9F/1F5D212E-C7AE-45E2-99E2-E6611F35F63A.png)
![](%E7%B3%BB%E7%BB%9F/B997FDA9-DBDA-43CF-A698-F12A45CE5F23.png)
为什么加了⼀个信号量     flag ，就实现了公平竞争？
对⽐⽅案⼀的读者优先策略，可以发现，读者优先中只要后续有读者到达，读者就可以进⼊读者队列，  ⽽写者必须等待，直到没有读者到达。
没有读者到达会导致读者队列为空，即     rCount==0 ，此时写者才可以进⼊临界区执⾏写操作。
⽽这⾥ flag 的作⽤就是阻⽌读者的这种特殊权限（特殊权限是只要读者到达，就可以进⼊读者队列）。 
⽐如：开始来了⼀些读者读数据，它们全部进⼊读者队列，此时来了⼀个写者，执⾏ P(falg) 操作，使得后续到来的读者都阻塞在 flag 上，不能进⼊读者队列，这会使得读者队列逐渐为空，即 rCount 减为0。
这个写者也不能⽴⻢开始写（因为此时读者队列不为空），会阻塞在信号量 wDataMutex 上，读者队列中的读者全部读取结束后，最后⼀个读者进程执⾏ V(wDataMutex) ，唤醒刚才的写者，写者则继续开始进⾏写操作。

### 死锁
##### 死锁发生条件：
死锁只有同时满⾜以下四个条件才会发⽣：
* 互斥条件：多个线程不能同时使⽤同⼀个资源； 
* 持有并等待条件：线程  A 在等待资源 2 的同时并不会释放⾃⼰已经持有的资源 1； 
* 不可剥夺条件：当线程已经持有了资源  ，在⾃⼰使⽤完之前不能被其他线程获取； 
* 环路等待条件：在死锁发⽣的时候，两个线程获取资源的顺序构成了环形链；
##### 利⽤⼯具排查死锁问题：
 Java 程序：使⽤ jstack ⼯具，它是 jdk ⾃带的线程堆栈分析⼯具。 
Linux下 C 代码：使⽤ pstack  + gdb ⼯具来定位死锁问题。
##### 避免死锁问题的发⽣
产⽣死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。避免死锁问题就只需要破环其中⼀个条件就可以，最常⻅的并且可⾏的就是使⽤资源有序分配法，来破环环路等待条件。
### 悲观锁与乐观锁
##### 互斥锁与⾃旋锁
###### 互斥锁与⾃旋锁区别：
* 互斥锁加锁失败后，线程会释放 CPU ，给其他线程； 
* ⾃旋锁加锁失败后，线程会忙等待，直到它拿到锁；
###### 互斥锁：
互斥锁是⼀种「独占锁」，⽐如当线程 A 加锁成功后，此时互斥锁已经被线程 A 独占了，只要线程 A 没有释放⼿中的锁，线程 B 加锁就会失败，于是就会释放 CPU 让给其他线程，既然线程 B 释放掉了 CPU，⾃然线程 B 加锁的代码就会被阻塞。
对于互斥锁加锁失败⽽阻塞的现象，是由操作系统内核实现的。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执⾏。
![](%E7%B3%BB%E7%BB%9F/CBE37669-AD5B-4AE9-84C5-60767D4E09EA.png)
互斥锁加锁失败时，性能开销成本：
会有两次线程上下⽂切换的成本：
* 当线程加锁失败时，内核会把线程的状态从「运⾏」状态设置为「睡眠」状态，然后把 CPU 切换给其他线程运⾏；
* 接着，当锁被释放时，之前「睡眠」状态的线程会变为「就绪」状态，然后内核会在合适的时间，把CPU 切换给该线程运⾏。
**如果你能确定被锁住的代码执⾏时间很短，就不应该⽤互斥锁，⽽应该选⽤⾃旋锁，否则使⽤互斥锁。**
###### ⾃旋锁
⾃旋锁是通过 CPU 提供的	CAS  函数（Compare And Swap），在「⽤户态」完成加锁和解锁操作，不会主动产⽣线程上下⽂切换
加锁过程：
        第⼀步，查看锁的状态，如果锁是空闲的，则执⾏第⼆步； 
        第⼆步，将锁设置为当前线程持有；
CAS 函数就把这两个步骤合并成⼀条硬件级指令，形成**原⼦指令**，这样就保证了这两个步骤是不可分割的，要么⼀次性执⾏完两个步骤，要么两个步骤都不执⾏。
使⽤⾃旋锁的时候，当发⽣多线程竞争锁的情况，加锁失败的线程会「忙等待」，直到它拿到锁。这⾥的「忙等待」可以⽤     while  循环等待实现，不过最好是使⽤ CPU 提供的   PAUSE  指令来实现「忙等待」，因为可以减少循环等待时的耗电量。
⾃旋锁是最⽐较简单的⼀种锁，⼀直⾃旋，利⽤ CPU 周期，直到锁可⽤。**需要注意，在单核 CPU 上，需要抢占式的调度器（即不断通过时钟中断⼀个线程，运⾏其他线程）。否则，⾃旋锁在单 CPU 上⽆法使⽤，因为⼀个⾃旋的线程永远不会放弃 CPU**。
⾃旋锁开销少，在多核系统下⼀般不会主动产⽣线程切换，适合异步、协程等在⽤户态切换请求的编程⽅式，但如果被锁住的代码执⾏时间过⻓，⾃旋的线程会⻓时间占⽤ CPU 资源，所以⾃旋的时间和被锁住的代码执⾏的时间是成「正⽐」的关系。
**互斥锁与⾃旋锁是锁的最基本处理⽅式，更⾼级的锁都会选择其中⼀个来实现**。
##### 读写锁
读写锁适⽤于能明确区分读操作和写操作的场景。 
读写锁的⼯作原理是：
        * 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这⼤⼤提⾼了共享资源的访问效率，因为「读锁」是⽤于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。 
        * 但是，⼀旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，⽽且其他写线程的获取写锁的操作也会被阻塞。
读写锁在读多写少的场景，能发挥出优势。 
另外，根据实现的不同，读写锁可以分为「读优先锁」和「写优先锁」。
既然不管优先读锁还是写锁，对⽅可能会出现饿死问题。**公平读写锁**⽐较简单的⼀种⽅式是：⽤队列把获取锁的线程排队，不管是写线程还是读线程都按照先进先出的原则加锁即可，这样读线程仍然可以并发，也不会出现「饥饿」的现象。
##### 乐观锁与悲观锁
互斥锁、⾃旋锁、读写锁，都是属于悲观锁。
悲观锁认为，多线程同时修改共享资源的概率⽐较⾼，于是很容易出现冲突，所以访问共享资源前，先要上锁。如果多线程同时修改共享资源的概率⽐较低，就可以采⽤乐观锁。
乐观锁假定冲突的概率很低，它的⼯作⽅式是：先修改完共享资源，再验证这段时间内有没有发⽣冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作。
放弃后如何重试，这跟业务场景息息相关。
**只有在冲突概率⾮常低，且加锁成本⾮常⾼的场景时，才考虑使⽤乐观锁。**

## 调度算法
### 进程调度算法
进程调度算法也称 CPU 调度算法。
##### 发⽣ CPU 调度的情况：
1.  当进程从运⾏状态转到等待状态；
2.  当进程从运⾏状态转到就绪状态；
3.  当进程从等待状态转到就绪状态；
4.  当进程从运⾏状态转到终⽌状态；
##### 常⻅的调度算法：
* 先来先服务调度算法 （First Come First Severd, FCFS）：
每次从就绪队列选择最先进⼊队列的进程，然后⼀直运⾏，直到进程退出或被阻塞，才会继续从队列中选择第⼀个进程接着运⾏。
* 最短作业优先调度算法 （Shortest Job First, SJF）：
优先选择运⾏时间最短的进程来运⾏。
* ⾼响应⽐优先调度算法 （Highest Response Ratio Next, HRRN）：
每次进⾏进程调度时，先计算「响应⽐优先级」，然后把「响应⽐优先级」最⾼的进程投⼊运⾏，「响应⽐优先级」的计算公式：
![](%E7%B3%BB%E7%BB%9F/9718BC8E-1F56-4443-81A5-F2E4C4A707C2.png)
* 时间⽚轮转调度算法 （Round Robin, RR）：
每个进程被分配⼀个时间段，称为时间⽚（Quantum），即允许该进程在该时间段中运⾏。
如果时间⽚⽤完，进程还在运⾏，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外⼀个进程；
如果该进程在时间⽚结束前阻塞或结束，则 CPU ⽴即进⾏切换；
通常时间⽚设为20ms~50ms  通常是⼀个⽐较合理的折中值。
* 最⾼优先级调度算法 （Highest Priority First，HPF）：
进程的优先级可以分为，静态优先级或动态优先级：
        静态优先级：创建进程时候，就已经确定了优先级了，然后整个运⾏时间优先级都不会变化； 
       动态优先级：根据进程的动态变化调整优先级，⽐如如果进程运⾏时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升⾼其优先级，也就是随着时间的推移增加等待进程的优先级。
该算法也有两种处理优先级⾼的⽅法，⾮抢占式和抢占式：
       ⾮抢占式：当就绪队列中出现优先级⾼的进程，运⾏完当前进程，再选择优先级⾼的进程。 
       抢占式：当就绪队列中出现优先级⾼的进程，当前进程挂起，调度优先级⾼的进程运⾏。
* 多级反馈队列调度算法（Multilevel Feedback Queue）：
是「时间⽚轮转算法」和「最⾼优先级算法」的综合和发展。
![](%E7%B3%BB%E7%BB%9F/621344AD-4DC9-4067-B1FC-E650715E4340.png)
设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从⾼到低，同时优先级越⾼时间⽚越短；
新的进程会被放⼊到第⼀级队列的末尾，按先来先服务的原则排队等待被调度，如果在第⼀级队列规定的时间⽚没运⾏完成，则将其转⼊到第⼆级队列的末尾，以此类推，直⾄完成；
当较⾼优先级的队列为空，才调度较低优先级的队列中的进程运⾏。如果进程运⾏时，有新进程进⼊较⾼优先级的队列，则停⽌当前运⾏的进程并将其移⼊到原队列末尾，接着让较⾼优先级的进程运⾏；

### 内存⻚⾯置换算法
当 CPU 访问的⻚⾯不在物理内存时，便会产⽣⼀个缺⻚中断，请求操作系统将所缺⻚调⼊到物理内存。那它与⼀般中断的主要区别在于：
* 缺⻚中断在指令执⾏「期间」产⽣和处理中断信号，⽽⼀般中断在⼀条指令执⾏「完成」后检查和处理中断信号。
* 缺⻚中断返回到该指令的开始重新执⾏「该指令」，⽽⼀般中断返回回到该指令的「下⼀个指令」执⾏。
###### 缺页中断处理流程：
![](%E7%B3%BB%E7%BB%9F/EB86173B-78F7-4ECB-9306-69E756CE51C3.png)
⻚⾯置换算法的功能是：当出现缺⻚异常，需调⼊新⻚⾯⽽内存已满时，选择被置换的物理⻚⾯，也就是说选择⼀个物理⻚⾯换出到磁盘，然后把需要访问的⻚⾯换⼊到物理⻚。
###### 常⻅的⻚⾯置换算法：
* 最佳⻚⾯置换算法（OPT） ：
置换在「未来」最⻓时间不访问的⻚⾯。
该算法实现需要计算内存中每个逻辑⻚⾯的「下⼀次」访问时间，然后⽐较，选择未来最⻓时间不访问的⻚⾯。
在实际系统中⽆法实现，因为程序访问⻚⾯时是动态的，我们是⽆法预知每个⻚⾯在「下⼀次」访问前的等待时间。
所以，最佳⻚⾯置换算法作⽤是为了衡量你的算法的效率，你的算法效率越接近该算法的效率，那么说明你的算法是⾼效的。
* 先进先出置换算法（FIFO）：
选择在内存驻留时间很⻓的⻚⾯进⾏中置换。
* 最近最久未使⽤的置换算法（LRU） ：
选择最⻓时间没有被访问的⻚⾯进⾏置换。
* 时钟⻚⾯置换算法（Lock） ：
把所有的⻚⾯都保存在⼀个类似钟⾯的「环形链表」中，⼀个表针指向最⽼的⻚⾯。 
当发⽣缺⻚中断时，算法⾸先检查表针指向的⻚⾯：
        如果它的访问位位是 0 就淘汰该⻚⾯，并把新的⻚⾯插⼊这个位置，然后把表针前移⼀个位置； 
        如果访问位是 1 就清除访问位，并把表针前移⼀个位置，重复这个过程直到找到了⼀个访问位为 0 的⻚⾯为⽌；
![](%E7%B3%BB%E7%BB%9F/25448FF4-24F0-49C0-B3A4-C2B5E864397B.png)
* 最不常⽤置换算法（LFU）：
当发⽣缺⻚中断时，选择「访问次数」最少的那个⻚⾯，并将其淘汰。

### 磁盘调度算法
* 先来先服务算法（First-Come，First-Served，FCFS） ：
先到来的请求，先被服务。
* 最短寻道时间优先算法 （Shortest Seek First，SSF）：
优先选择从当前磁头位置所需寻道时间最短的请求。
可能存在某些请求的饥饿，假设是⼀个动态的请求，可能磁头在⼀⼩块区域来回移动。
* 扫描算法（Scan）：
磁头在⼀个⽅向上移动，访问所有未完成的请求，直到磁头到达该⽅向上的最后的磁道，才调换⽅向，这就是扫描算法。
* 循环扫描算法 （Circular Scan, CSCAN ）：
只有磁头朝某个特定⽅向移动时，才处理磁道访问请求，⽽返回时直接快速移动⾄最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且返回中途不处理任何请求，该算法的特点，就是磁道只响应⼀个⽅向上的请求。
* LOOK 与 C-LOOK 算法
LOOK 算法：针对 SCAN 算法的优化。⼯作⽅式：磁头在每个⽅向上仅仅移动到最远的请求位置，然后⽴即反向移动，⽽不需要移动到磁盘的最始端或最末端，反向移动的途中会响应请求。
C-LOOK算法：针对 C-SCAN 算法的优化。⼯作⽅式：磁头在每个⽅向上仅仅移动到最远的请求位置，然后⽴即反向移动，⽽不需要移动到磁盘的最始端或最末端，反向移动的途中不会响应请求。

## 设备管理
#### 设备控制器
为了屏蔽设备之间的差异，每个设备都有⼀个叫设备控制器（Device Control）  的组件，⽐如硬盘有硬盘控制器、显示器有视频控制器等。
![](%E7%B3%BB%E7%BB%9F/F0983FD0-E67F-435C-9A9C-6B6953DF6842.png)
设备控制器⾥有芯⽚，它可执⾏⾃⼰的逻辑，也有⾃⼰的寄存器，⽤来与 CPU 进⾏通信，⽐如：
1）通过写⼊这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执⾏某些其他操作。
2）通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收⼀个新的命令等。
##### 控制器的寄存器：
![](%E7%B3%BB%E7%BB%9F/6D8780C2-8421-4D9B-B32F-66E95E7CB90B.png)
* 数据寄存器，CPU 向 I/O 设备写⼊需要传输的数据，⽐如要打印的内容是「Hello」，CPU 就要先发送⼀个 H 字符给到对应的 I/O 设备。
* 命令寄存器，CPU 发送⼀个命令，告诉 I/O 设备，要进⾏输⼊/输出操作，于是就会交给 I/O 设备去⼯作，任务完成后，会把状态寄存器⾥⾯的状态标记为完成。
* 状态寄存器，⽬的是告诉 CPU ，现在已经在⼯作或⼯作已经完成，如果已经在⼯作状态，CPU 再发送数据或者命令过来，都是没有⽤的，直到前⾯的⼯作已经完成，状态寄存标记成已完成，CPU 才能发送下⼀个字符和命令。
CPU 通过读写设备控制器中的寄存器控制设备，这可⽐ CPU 直接控制输⼊输出设备，要⽅便和标准很多。
##### 块设备和字符设备
 输⼊输出设备可分为两⼤类  ：块设备（Block Device）和字符设备（Character Device）。
* 块设备，把数据存储在固定⼤⼩的块中，每个块有⾃⼰的地址，硬盘、USB 是常⻅的块设备。 
* 字符设备，以字符为单位发送或接收⼀个字符流，字符设备是不可寻址的，也没有任何寻道操作，⿏标是常⻅的字符设备。
> 块设备通常传输的数据量会⾮常⼤，于是控制器设⽴了⼀个可读写的数据缓冲区。  
> 1）CPU 写⼊数据到控制器的缓冲区时，当缓冲区的数据囤够了⼀部分，才会发给设备。   
> 2）CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了⼀部分，才拷⻉到内存。  
> 这样做是为了，减少对设备的频繁操作。  
##### CPU 如何与设备的控制寄存器和数据缓冲区进⾏通信：
存在两个⽅法：
* 端⼝ I/O，每个控制寄存器被分配⼀个 I/O 端⼝，可以通过特殊的汇编指令操作这些寄存器，⽐如 in/out  类似的指令。
* 内存映射 I/O，将所有控制寄存器映射到内存空间中，这样就可以像读写内存⼀样读写数据缓冲区。
![](%E7%B3%BB%E7%BB%9F/77474F57-9647-440E-83C0-786CAB48F7B8.png)

#### I/O控制方式
每种设备都有⼀个设备控制器，控制器相当于⼀个⼩ CPU，它可以⾃⼰处理⼀些事情，但有个问题是，当 CPU 给设备发送了⼀个指令，让设备控制器去读设备的数据，它读完的时候，要怎么通知CPU 呢？
——DMA（Direct Memory Access）  功能，它可以使得设备在CPU 不参与的情况下，能够⾃⾏完成把设备 I/O 数据放⼊到内存。那要实现 DMA 功能要有  「DMA 控制器」硬件的⽀持。
![](%E7%B3%BB%E7%BB%9F/FDAD8D84-F074-475B-9281-685E58DA262F.png)
DMA 的⼯作⽅式如下：
* CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地⽅就可以了；
* 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；
* 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出⼀个确认成功的信号到DMA 控制器；
* DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接⽤内存⾥⾯现成的数据了；

#### 设备驱动程序
虽然设备控制器屏蔽了设备的众多细节，但每种设备的控制器的寄存器、缓冲区等使⽤模式都是不同的，所以为了屏蔽「设备控制器」的差异，引⼊了设备驱动程序。
![](%E7%B3%BB%E7%BB%9F/246C3113-B199-4DBA-9023-91B5020F4BB3.png)
设备驱动程序属于操作系统的⼀部分，设备驱动程序是⾯向设备控制器的代码，它发出操控设备控制器的指令后，才可以操作设备控制器。
不同的设备控制器虽然功能不同，但是**设备驱动程序会提供统⼀的接⼝给操作系统**，这样不同的设备驱动程序，就可以以相同的⽅式接⼊操作系统。
设备完成了事情，则会发送中断来通知操作系统。那操作系统就需要有⼀个地⽅来处理这个中断，这个地⽅也就是在设备驱动程序⾥，它会及时响应控制器发来的中断请求，并根据这个中断的类型调⽤响应的**中断处理程序**进⾏处理。
![](%E7%B3%BB%E7%BB%9F/A745EB54-BEF1-45EC-89C7-E64C03F1F8F4.png)
中断处理程序的处理流程：
1.  在 I/O 时，设备控制器如果已经准备好数据，则会通过中断控制器向 CPU 发送中断请求；
2.  保护被中断进程的 CPU 上下⽂；
3.  转⼊相应的设备中断处理函数；
4.  进⾏中断处理；
5.  恢复被中断进程的上下⽂；

#### 通⽤块层
对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过⼀个统⼀的通⽤块层，来管理不同的块设备。
通⽤块层是处于⽂件系统和磁盘驱动中间的⼀个块设备抽象层，它主要有两个功能：
* 第⼀个功能，向上为⽂件系统和应⽤程序，提供访问块设备的标准接⼝，向下把各种不同的磁盘设备抽象为统⼀的块设备，并在内核层⾯，提供⼀个框架来管理这些设备的驱动程序；
* 第⼆功能，通⽤层还会给⽂件系统和应⽤程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等⽅式，也就是 I/O 调度，主要⽬的是为了提⾼磁盘读写的效率。
###### Linux 内存⽀持 5 种 I/O 调度算法：
* 没有调度算法：
对⽂件系统和应⽤程序的 I/O 做任何处理，这种算法常⽤在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。
* 先⼊先出调度算法：
先进⼊ I/O 调度队列的 I/O 请求先发⽣。
* 完全公平调度算法：
⼤部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了⼀个
I/O 调度队列，并按照时间⽚来均匀分布每个进程的 I/O 请求。
* 优先级调度：
优先级⾼的 I/O 请求先发⽣，  它适⽤于运⾏⼤量进程的系统，像是桌⾯环境、多媒体应⽤等。
* 最终期限调度算法：
分别为读、写请求创建了不同的 I/O 队列，这样可以提⾼机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适⽤于在 I/O 压⼒⽐较⼤的场景，⽐如数据库等。

#### 存储系统 I/O 软件分层
可以把 Linux 存储系统的 I/O 由上到下可以分为三个层次，分别是⽂件系统层、通⽤块层、设备层：
![](%E7%B3%BB%E7%BB%9F/A43BE8F0-1BEA-4F9D-8E66-FF0353F33AA0.png)
这三个层次的作⽤是：
* ⽂件系统层，包括虚拟⽂件系统和其他⽂件系统的具体实现，它向上为应⽤程序统⼀提供了标准的⽂件访问接⼝，向下会通过通⽤块层来存储和管理磁盘数据。
* 通⽤块层，包括块设备的 I/O 队列和 I/O 调度器，它会对⽂件系统的 I/O 请求进⾏排队，再通过 I/O调度器，选择⼀个 I/O 发给下⼀层的设备层。
* 设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。
存储系统的 I/O 是整个系统最慢的⼀个环节，所以 Linux 提供了不少缓存机制来提⾼ I/O 的效率：
1）为了提⾼⽂件访问的效率，会使⽤**⻚缓存、索引节点缓存、⽬录项缓存**等多种缓存机制，⽬的是为了减少对块设备的直接调⽤。
2）为了提⾼块设备的访问效率，  会使⽤**缓冲区**，来缓存块设备的数据。

#### 键盘敲⼊字⺟时，期间发⽣了什么？
当⽤户输⼊了键盘字符，键盘控制器就会产⽣扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送中断请求。
CPU 收到中断请求后，操作系统会保存被中断进程的 CPU 上下⽂，然后调⽤键盘的中断处理程序。 
键盘的中断处理程序是在键盘驱动程序初始化时注册的，那键盘中断处理函数的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到⽤户在键盘输⼊的字符，如果输⼊的字符是显示字符，那就会把扫描码翻译成对应显示字符的  ASCII 码，⽐如⽤户在键盘输⼊的是字⺟ A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。
得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」，最后把「写缓冲区队列」的数据⼀个⼀个写⼊到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕⾥。
显示出结果后，恢复被中断进程的上下⽂。

## 网络系统
### Linux系统系统如何收发网络包
#### OSI网络模型
* 应⽤层，负责给应⽤程序提供统⼀的接⼝；
* 表示层，负责把数据转换成兼容另⼀个系统能识别的格式； 
* 会话层，负责建⽴、管理和终⽌表示层实体之间的通信会话； 
* 传输层，负责端到端的数据传输；
* ⽹络层，负责数据的路由、转发、分⽚；
* 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址； 
* 物理层，负责在物理⽹络中传输数据帧；
##### TCP/IP网络模型
* 应⽤层，负责向⽤户提供⼀组应⽤程序，⽐如 HTTP、DNS、FTP 等; 
* 传输层，负责端到端的通信，⽐如 TCP、UDP 等；
* ⽹络层，负责⽹络包的封装、分⽚、路由、转发，⽐如 IP、ICMP 等；
* ⽹络接⼝层，负责⽹络包在物理⽹络中的传输，⽐如⽹络包的封帧、 MAC 寻址、差错检测，以及通过⽹卡传输⽹络帧等；
#### Linux网络协议栈
应用层数据封装：
![](%E7%B3%BB%E7%BB%9F/A53C5ADE-396C-4DC7-8EB5-7C7653677648.png)
物理链路并不能传输任意⼤⼩的数据包，所以在以太⽹中，规定了最⼤传输单元（MTU）是 1500 字节，也就是规定了单次传输的最⼤ IP 包⼤⼩。
当⽹络包超过 MTU 的⼤⼩，就会在⽹络层分⽚，以确保分⽚后的 IP 包不会超过 MTU ⼤⼩，如果 MTU 越⼩，需要的分包就越多，那么⽹络吞吐能⼒就越差，相反的，如果 MTU 越⼤，需要的分包就越⼩，那么⽹络吞吐能⼒就越好。
![](%E7%B3%BB%E7%BB%9F/2A1239CB-8AB9-48A1-96BF-0DDD5799B44D.png)
![](%E7%B3%BB%E7%BB%9F/2B281EF7-8816-4856-BF13-92E7EBDA9481.png)
![](%E7%B3%BB%E7%BB%9F/909BC079-50E8-4B68-8269-5E952BC0374F.png)
#### Linux 接收⽹络包的流程
⽹卡是计算机⾥的⼀个硬件，专⻔负责接收和发送⽹络包，当⽹卡接收到⼀个⽹络包后，会通过 **DMA 技术**，将⽹络包放⼊到 Ring Buffer，这个是⼀个环形缓冲区。
那接收到⽹络包后，应该怎么告诉操作系统这个⽹络包已经到达了呢？
最简单的⼀种⽅式就是触发中断，也就是每当⽹卡收到⼀个⽹络包，就触发⼀个中断告诉操作系统。 
但是，这存在⼀个问题，在⾼性能⽹络场景下，⽹络包的数量会⾮常多，那么就会触发⾮常多的中断，要知道当 CPU 收到了中断，就会停下⼿⾥的事情，⽽去处理这些⽹络包，处理完毕后，才会回去继续其他事情，那么频繁地触发中断，则会导致 CPU ⼀直没玩没了的处理中断，⽽导致其他任务可能⽆法继续前进，从⽽影响系统的整体效率。
所以为了解决频繁中断带来的性能开销，Linux 内核在 2.6 版本中引⼊了**NAPI 机制**，它是混合「中断和轮询」的⽅式来接收⽹络包，它的核⼼概念就是**不采⽤中断的⽅式读取数据**，⽽是⾸先采⽤中断唤醒数据接收的服务程序，然后 poll 的⽅法来轮询数据。
⽐如，当有⽹络包到达时，⽹卡发起硬件中断，于是会执⾏⽹卡硬件中断处理函数，中断处理函数处理完需要「暂时屏蔽中断」，然后唤醒「软中断」来轮询处理数据，直到没有新数据时才恢复中断，这样⼀次中断处理多个⽹络包，于是就可以降低⽹卡中断带来的性能开销。
那软中断是怎么处理⽹络包的呢？它会从 Ring Buffer 中拷⻉数据到内核 struct sk_buff 缓冲区中，从⽽可以作为⼀个⽹络包交给⽹络协议栈进⾏逐层处理。
⾸先，会先进⼊到⽹络接⼝层，在这⼀层会检查报⽂的合法性，如果不合法则丢弃，合法则会找出该⽹络包的上层协议的类型，⽐如是 IPv4，还是 IPv6，接着再去掉帧头和帧尾，然后交给⽹络层。
到了⽹络层，则取出 IP 包，判断⽹络包下⼀步的⾛向，⽐如是交给上层处理还是转发出去。当确认这个⽹络包要发送给本机后，就会从 IP 头⾥看看上⼀层协议的类型是 TCP 还是 UDP，接着去掉 IP 头，然后交给传输层。
传输层取出 TCP 头或 UDP 头，根据四元组「源 IP、源端⼝、⽬的 IP、⽬的端⼝」  作为标识，找出对应的 Socket，并把数据拷⻉到 Socket 的接收缓冲区。
最后，应⽤层程序调⽤ Socket 接⼝，从内核的 Socket 接收缓冲区读取新到来的数据到应⽤层。 
⾄此，⼀个⽹络包的接收过程就已经结束了。下图左边部分是⽹络包接收的流程，右边部分是⽹络包发送的流程：
![](%E7%B3%BB%E7%BB%9F/CD91C372-6A79-46E8-AE6A-61BED4BB077A.png)
#### Linux 发送⽹络包的流程
如上图的右半部分，发送⽹络包的流程正好和接收流程相反。
⾸先，应⽤程序会调⽤ Socket 发送数据包的接⼝，由于这个是系统调⽤，所以会从⽤户态陷⼊到内核态中的 Socket 层，Socket 层会将应⽤层数据拷⻉到 Socket 发送缓冲区中。
接下来，⽹络协议栈从 Socket 发送缓冲区中取出数据包，并按照 TCP/IP 协议栈从上到下逐层处理。 
如果使⽤的是 TCP 传输协议发送数据，那么会在传输层增加 TCP 包头，然后交给⽹络层，⽹络层会给数据包增加 IP 包，然后通过查询路由表确认下⼀跳的 IP，并按照 MTU ⼤⼩进⾏分⽚。
分⽚后的⽹络包，就会被送到⽹络接⼝层，在这⾥会通过 ARP 协议获得下⼀跳的 MAC 地址，然后增加帧头和帧尾，放到发包队列中。
这⼀些准备好后，会触发软中断告诉⽹卡驱动程序，这⾥有新的⽹络包需要发送，最后驱动程序通过DMA，从发包队列中读取⽹络包，将其放⼊到硬件⽹卡的队列中，随后物理⽹卡再将它发送出去。

### 零拷贝
#### DMA
DMA：Direct Memory Access直接内存访问
在进⾏ I/O 设备和内存的数据传输的时候，数据搬运的⼯作全部交给DMA 控制器，⽽ CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。
![](%E7%B3%BB%E7%BB%9F/A02817F5-73CE-40B9-8FF2-24B01C0C2D4D.png)
具体过程：
* ⽤户进程调⽤ read ⽅法，向操作系统发出 I/O 请求，请求读取数据到⾃⼰的内存缓冲区中，进程进⼊阻塞状态；
* 操作系统收到请求后，进⼀步将 I/O 请求发送 DMA，然后让 CPU 执⾏其他任务； 
* DMA 进⼀步将 I/O 请求发送给磁盘；
* 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知⾃⼰缓冲区已满；
* **DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷⻉到内核缓冲区中，此时不占⽤ CPU，CPU可以执⾏其他任务**；
* 当 DMA 读取了⾜够多的数据，就会发送中断信号给 CPU；
* CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷⻉到⽤户空间，系统调⽤返回； 
在整个数据传输的过程，CPU 不再参与数据搬运的⼯作，⽽是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪⾥传输到哪⾥，都需要 CPU 来告诉 DMA 控制器。 
早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备⾥⾯都有⾃⼰的 DMA 控制器。
#### 传统的⽂件传输
如果服务端要提供⽂件传输的功能，我们能想到的最简单的⽅式是：将磁盘上的⽂件读取出来，然后通过⽹络协议发送给客户端。
代码通常如下，⼀般会需要两个系统调⽤： 
```
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```
![](%E7%B3%BB%E7%BB%9F/676E9F7B-E387-4125-9B17-A25451EF375C.png)
要想提⾼⽂件传输的性能，就需要减少「⽤户态与内核态的上下⽂切换」和「内存拷⻉」的次数。
要想减少上下⽂切换到次数，就要减少系统调⽤的次数。
因为⽂件传输的应⽤场景中，在⽤户空间我们并不会对数据「再加⼯」，所以数据实际上可以不⽤搬运到⽤户空间，因此⽤户的缓冲区是没有必要存在的。
#### 零拷贝实现
零拷⻉技术实现的⽅式通常有 2 种：mmap + write 、sendfile
* mmap + write
read()  系统调⽤的过程中会把内核缓冲区的数据拷⻉到⽤户的缓冲区⾥，于是为了减少这⼀步开销，我们可以⽤ mmap()  替换 read()  系统调⽤函数。
```
buf = mmap(file, len); 
write(sockfd, buf, len);
```
mmap()  系统调⽤函数会直接把内核缓冲区⾥的数据「映射」到⽤户空间，这样，操作系统内核与⽤户空间就不需要再进⾏任何的数据拷⻉操作。
![](%E7%B3%BB%E7%BB%9F/824E9BC8-EA45-4DC6-852A-079B2A24974C.png)
具体过程如下：
1）应⽤进程调⽤了 mmap()  后，DMA 会把磁盘的数据拷⻉到内核的缓冲区⾥。接着，应⽤进程跟操作系统内核「共享」这个缓冲区；
2）应⽤进程再调⽤ write() ，操作系统直接将内核缓冲区的数据拷⻉到 socket 缓冲区中，这⼀切都发⽣在内核态，由 CPU 来搬运数据；
3）最后，把内核的 socket 缓冲区⾥的数据，拷⻉到⽹卡的缓冲区⾥，这个过程是由 DMA 搬运的。 
我们可以得知，通过使⽤     mmap()  来代替     read() ，  可以减少⼀次数据拷⻉的过程。
* sendfile
在 Linux 内核版本 2.1 中，提供了⼀个专⻔发送⽂件的系统调⽤函数 sendfile() ，函数形式如下： 
```
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
//它的前两个参数分别是⽬的端和源端的⽂件描述符，后⾯两个参数是源端的偏移量和复制数据的⻓度，返回值是实际复制数据的⻓度。
```
⾸先，它可以替代前⾯的read()  和 write()  这两个系统调⽤，这样就可以减少⼀次系统调⽤，也就减少了 2 次上下⽂切换的开销。
其次，该系统调⽤可以直接把内核缓冲区⾥的数据拷⻉到 socket 缓冲区⾥，不再拷⻉到⽤户态，这样就只有 2 次上下⽂切换，和 3 次数据拷⻉。如下图：
![](%E7%B3%BB%E7%BB%9F/A376D1EA-4E48-4CEF-A04A-563C6A1800F2.png)
但是这还不是真正的零拷⻉技术，如果⽹卡⽀持 SG-DMA（The Scatter-Gather Direct Memory Access） 技术（和普通的 DMA 有所不同），我们可以进⼀步减少通过 CPU 把内核缓冲区⾥的数据拷⻉到 socket 缓冲区的过程。
从 Linux 内核2.4  版本开始起，对于⽀持⽹卡⽀持 SG-DMA 技术的情况下，     sendfile()  系统调⽤的过程发⽣了点变化，具体过程如下：
1）第⼀步，通过 DMA 将磁盘上的数据拷⻉到内核缓冲区⾥；
2）第⼆步，缓冲区描述符和数据⻓度传到 socket 缓冲区，这样⽹卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷⻉到⽹卡的缓冲区⾥，此过程不需要将数据从操作系统内核缓冲区拷⻉到 socket缓冲区中，这样就减少了⼀次数据拷⻉。
所以，这个过程之中，只进⾏了 2 次数据拷⻉，如下图：
![](%E7%B3%BB%E7%BB%9F/334408E0-53CF-43A6-9883-B3D00BCA0D80.png)
这就是所谓的零拷⻉（Zero-copy）技术，因为我们没有在内存层⾯去拷⻉数据，也就是说全程没有通过CPU 来搬运数据，所有的数据都是通过 DMA 来进⾏传输的。
零拷⻉技术的⽂件传输⽅式相⽐传统⽂件传输的⽅式，减少了 2 次上下⽂切换和数据拷⻉次数，只需要 2 次上下⽂切换和数据拷⻉次数，就可以完成⽂件的传输，⽽且 2 次的数据拷⻉过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。
所以，总体来看，零拷⻉技术可以把⽂件传输的性能提⾼⾄少⼀倍以上。
#### PageCache 
前⾯说到的⽂件传输过程，其中第⼀步都是先需要先把磁盘⽂件数据拷⻉「内核缓冲区」⾥，这个「内核缓冲区」实际上是磁盘⾼速缓存（PageCache）。
PageCache 的优点主要是两个：
        缓存最近被访问的数据； 
        预读功能；
这两个做法，将⼤⼤提⾼读写磁盘的性能。
**但是，在传输⼤⽂件（GB 级别的⽂件）的时候，PageCache 会不起作⽤，那就⽩⽩浪费 DMA 多做的⼀ 次数据拷⻉，造成性能的降低，即使使⽤了 PageCache 的零拷⻉也会损失性能。**
所以，针对⼤⽂件的传输，不应该使⽤ PageCache，也就是说不应该使⽤零拷⻉技术，因为可能由于PageCache 被⼤⽂件占据，⽽导致「热点」⼩⽂件⽆法利⽤到 PageCache，这样在⾼并发的环境下，会带来严重的性能问题。
#### ⼤⽂件传输方式
在⾼并发的场景下，针对⼤⽂件的传输的⽅式，应该使⽤「异步 I/O + 直接 I/O」来替代零拷⻉技术。
直接 I/O 应⽤场景常⻅的两种：
* 应⽤程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启；
* 传输⼤⽂件的时候，由于⼤⽂件难以命中 PageCache 缓存，⽽且会占满 PageCache 导致「热点」⽂件⽆法充分利⽤缓存，从⽽增⼤了性能开销，因此，这时应该使⽤直接 I/O。
另外，由于直接 I/O 绕过了 PageCache，就⽆法享受内核的这两点的优化：
* 内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「合并」成⼀个更⼤的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；
* 内核也会「预读」后续的 I/O 请求放在 PageCache 中，⼀样是为了减少对磁盘的操作； 
于是，传输⼤⽂件的时候，使⽤「异步 I/O + 直接 I/O」了，就可以⽆阻塞地读取⽂件了。 
所以，传输⽂件的时候，我们要根据⽂件的⼤⼩来使⽤不同的⽅式：
* 传输⼤⽂件的时候，使⽤「异步 I/O + 直接 I/O」； 
* 传输⼩⽂件的时候，则使⽤「零拷⻉技术」；

### I/O多路复用select/poll/epoll
⼀个进程虽然任⼀时刻只能处理⼀个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉⻓来看，多个请求复⽤了⼀个进程，这就是多路复⽤，这种思想很类似⼀个 CPU 并发多个进程，所以也叫做时分多路复⽤。
我们熟悉的 select/poll/epoll 内核提供给⽤户态的多路复⽤系统调⽤，**进程可以通过⼀个系统调⽤函数从内核中获取多个事件**。
select/poll/epoll 是如何获取⽹络事件的呢？在获取事件时，先把所有连接（⽂件描述符）传给内核，再由内核返回产⽣了事件的连接，然后在⽤户态中再处理这些连接对应的请求即可。
#### select/poll
select 实现多路复⽤的⽅式是，将已连接的 Socket 都放到⼀个⽂件描述符集合，然后调⽤ select 函数将⽂件描述符集合拷⻉到内核⾥，让内核来检查是否有⽹络事件产⽣，检查的⽅式很粗暴，就是通过遍历⽂件描述符集合的⽅式，当检查到有事件产⽣后，将此 Socket 标记为可读或可写， 接着再把整个⽂件描述符集合拷⻉回⽤户态⾥，然后⽤户态还需要再通过遍历的⽅法找到可读或可写的 Socket，然后再对其处理。
对于 select 这种⽅式，需要进⾏ 2 次「遍历」⽂件描述符集合，⼀次是在内核态⾥，⼀个次是在⽤户态⾥ ，⽽且还会发⽣ 2 次「拷⻉」⽂件描述符集合，先从⽤户空间传⼊内核空间，由内核修改后，再传出到⽤户空间中。
select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在 Linux 系统中，由内核中的 FD_SETSIZE 限制，  默认最⼤值为1024 ，只能监听 0~1023 的⽂件描述符。
poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。
但是 poll 和 select 并没有太⼤的本质区别，**都是使⽤「线性结构」存储进程关注的 Socket 集合，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合**，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。
#### epoll
epoll 通过两个⽅⾯，很好解决了 select/poll 的问题。
* 第⼀点，epoll 在内核⾥使⽤红⿊树来跟踪进程所有待检测的⽂件描述字，把需要监控的 socket 通过 epoll_ctl()  函数加⼊内核中的红⿊树⾥，红⿊树是个⾼效的数据结构，增删查⼀般时间复杂度是O(logn) ，通过对这棵⿊红树进⾏操作，这样就不需要像 select/poll 每次操作时都传⼊整个 socket 集合，只需要传⼊⼀个待检测的 socket，减少了内核和⽤户空间⼤量的数据拷⻉和内存分配。
* 第⼆点， epoll 使⽤事件驱动的机制，内核⾥维护了⼀个链表来记录就绪事件，当某个 socket 有事件发⽣时，通过回调函数内核会将其加⼊到这个就绪事件列表中，当⽤户调⽤ epoll_wait() 函数时，只会返回有事件发⽣的⽂件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，⼤⼤提⾼了检测的效率。
![](%E7%B3%BB%E7%BB%9F/93E6CB20-1ED9-4095-B7F9-EE2763AD1EE6.png)
epoll 的⽅式即使监听的 Socket 数量越多的时候，效率不会⼤幅度降低，能够同时监听的 Socket 的数⽬也⾮常的多了，上限就为系统定义的进程打开的最⼤⽂件描述符个数。因⽽，**epoll 被称为解决 C10K 问题的利器**。
epoll ⽀持两种事件触发模式，分别是边缘触发（edge-triggered，ET）和⽔平触发（level-triggered， LT ）。
* 使⽤边缘触发模式时，当被监控的 Socket 描述符上有可读事件发⽣时，服务器端只会从 epoll_wait 中苏醒⼀次，即使进程没有调⽤ read 函数从内核读取数据，也依然只苏醒⼀次，因此我们程序要保证⼀次性将内核缓冲区的数据读取完；
* 使⽤⽔平触发模式时，当被监控的 Socket 上有可读事件发⽣时，服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束，⽬的是告诉我们有数据需要读取；
边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和 write ）返回错误，错误类型为 EAGAIN 或EWOULDBLOCK 。
⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数，系统调⽤也是有⼀定的开销的的，毕竟也存在上下⽂的切换。
select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发模式。

### ⾼性能⽹络模式：Reactor 和 Proactor
Reactor 模式也叫 Dispatcher  模式，即  I/O 多路复⽤监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程。
Reactor 模式主要由 Reactor 和处理资源池这两个核⼼部分组成，它俩负责的事情如下：
         * Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件； 
         * 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；
Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：
         * Reactor 的数量可以只有⼀个，也可以有多个；
         * 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；
将上⾯的两个因素排列组设⼀下，理论上就可以有 4 种⽅案选择：
         * 单 Reactor 单进程 / 线程； 
         * 单 Reactor 多进程 / 线程； 
         * 多 Reactor 单进程 / 线程（不仅复杂⽽且也没有性能优势，因此实际中并没有应⽤）； 
         * 多 Reactor 多进程 / 线程；
#### Reactor
##### 单 Reactor 单进程 / 线程
⼀般来说，C 语⾔实现的是「单 Reactor 单进程」的⽅案，因为 C 语编写完的程序，运⾏后就是⼀个独⽴的进程，不需要在进程中再创建线程。
⽽ Java 语⾔实现的是「单 Reactor 单线程」的⽅案，因为 Java 程序是跑在 Java 虚拟机这个进程上⾯的，虚拟机中有很多线程，我们写的 Java 程序只是其中的⼀个线程⽽已。
「单 Reactor 单进程」的⽅案示意图：
![](%E7%B3%BB%E7%BB%9F/2771B2A4-6052-4199-ABF9-B94FA13EB01D.png)
进程⾥有  Reactor、Acceptor、Handler 这三个对象：
         * Reactor 对象的作⽤是监听和分发事件； 
         * Acceptor 对象的作⽤是获取连接； 
         * Handler 对象的作⽤是处理业务；
对象⾥的 select、accept、read、send 是系统调⽤函数，dispatch 和  「业务处理」是需要完成的操作， 其中 dispatch 是分发事件操作。
「单 Reactor 单进程」⽅案介绍：
         * Reactor 对象通过 select （IO 多路复⽤接⼝）  监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
         * 如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept ⽅法  获取连接，并创建⼀个 Handler 对象来处理后续的响应事件；
         * 如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应； 
         * Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。
单 Reactor 单进程的⽅案因为全部⼯作都在同⼀个进程内完成，所以实现起来⽐较简单，不需要考虑进程间通信，也不⽤担⼼多进程竞争。
但是，这种⽅案存在 2 个缺点：
         * 第⼀个缺点，因为只有⼀个进程，⽆法充分利⽤多核 CPU 的性能；
         * 第⼆个缺点，Handler 对象在业务处理时，整个进程是⽆法处理其他连接的事件的，如果业务处理耗时⽐较⻓，那么就造成响应的延迟；
所以，单 Reactor 单进程的⽅案**不适⽤计算机密集型的场景，只适⽤于业务处理⾮常快速的场景**。
##### 单 Reactor 多线程 / 多进程
![](%E7%B3%BB%E7%BB%9F/95C172C8-03D7-4598-B1F7-FC43AE0B2911.png)
###### 「单 Reactor 多线程」⽅案说明：
         * Reactor 对象通过 select （IO 多路复⽤接⼝）  监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
         * 如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept ⽅法  获取连接，并创建⼀个 Handler 对象来处理后续的响应事件；
         * 如果不是连接建⽴事件，  则交由当前连接对应的 Handler 对象来进⾏响应； 
         * Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给⼦线程⾥的 Processor 对象进⾏业务处理；
         * ⼦线程⾥的 Processor 对象就进⾏业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send ⽅法将响应结果发送给 client；
单 Reator 多线程的⽅案优势在于能够充分利⽤多核 CPU 的能，那既然引⼊多线程，那么⾃然就带来了多线程竞争资源的问题，需要在操作共享资源前加上互斥锁。
###### 「单 Reactor 多进程」⽅案
单 Reactor 多进程相⽐单 Reactor 多线程实现起来很麻烦，主要因为要考虑⼦进程 <-> ⽗进程的双向通信，并且⽗进程还得知道⼦进程要将数据发送给哪个客户端。
⽽多线程间可以共享数据，虽然要额外考虑并发问题，但是这远⽐进程间通信的复杂度低得多，因此实际应⽤中也看不到单 Reactor 多进程的模式。
另外，「单 Reactor」的模式还有个问题，**因为⼀个 Reactor 对象承担所有事件的监听和响应，⽽且只在主线程中运⾏，在⾯对瞬间⾼并发的场景时，容易成为性能的瓶颈的地⽅**。
##### 多 Reactor 多进程 / 线程
![](%E7%B3%BB%E7%BB%9F/B7188704-C0D7-4AB5-9E07-EA6AEFDF932F.png)
⽅案说明：
         * 主线程中的 MainReactor 对象通过 select 监控连接建⽴事件，收到事件后通过 Acceptor 对象中的accept 获取连接，将新的连接分配给某个⼦线程；
         * ⼦线程中的 SubReactor 对象将 MainReactor 对象分配的连接加⼊ select 继续进⾏监听，并创建⼀个Handler ⽤于处理连接的响应事件。
         * 如果有新的事件发⽣时，SubReactor 对象会调⽤当前连接对应的 Handler 对象来进⾏响应。 
         * Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。
多 Reactor 多线程的⽅案虽然看起来复杂的，但是实际实现时⽐单 Reactor 多线程的⽅案要简单的多，原因如下：
         * 主线程和⼦线程分⼯明确，主线程只负责接收新连接，⼦线程负责完成后续的业务处理。
         * 主线程和⼦线程的交互很简单，主线程只需要把新连接传给⼦线程，⼦线程⽆须返回数据，直接就可以在⼦线程将处理结果发送给客户端。

#### Proactor 
Proactor 是异步⽹络模型。
Reactor 和 Proactor 的区别：
         * Reactor 是⾮阻塞同步⽹络模式，感知的是就绪可读写事件。在每次感知到有事件发⽣（⽐如可读就绪事件）后，就需要应⽤进程主动调⽤ read ⽅法来完成数据的读取，也就是要应⽤进程主动将socket 接收缓存中的数据读到应⽤进程内存中，这个过程是同步的，读取完数据后应⽤进程才能处理 
数据。
         * Proactor 是异步⽹络模式， 感知的是已完成的读写事件。在发起异步读写请求时，需要传⼊数据缓冲区的地址（⽤来存放结果数据）等信息，这样系统内核才可以⾃动帮我们把数据的读写⼯作完成，这⾥的读写⼯作全程由操作系统来做，并不需要像 Reactor 那样还需要应⽤进程主动发起 read/write 
来读写数据，操作系统完成读写⼯作后，就会通知应⽤进程直接处理数据。
因此，Reactor 可以理解为「来了事件操作系统通知应⽤进程，让应⽤进程来处理」，⽽ Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应⽤进程」。这⾥的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这⾥的「处理」包含从驱动读取到内核以及从内核读取到⽤户空间。
Proactor 模式示意图：
![](%E7%B3%BB%E7%BB%9F/CED146C8-C95A-49E1-96B7-EC26AB05546E.png)
Proactor 模式的⼯作流程：
         * Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过Asynchronous Operation Processor 注册到内核；
         * Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
         * Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor； 
         * Proactor 根据不同的事件类型回调不同的 Handler 进⾏业务处理； 
         * Handler 完成业务处理；
在 Linux 下的异步 I/O 是不完善的，aio 系列函数是由 POSIX 定义的异步操作接⼝，不是真正的操作系统级别⽀持的，⽽是在⽤户空间模拟出来的异步，并且仅仅⽀持基于本地⽂件的 aio 异步操作，⽹络编程中的 socket 是不⽀持的，这也使得基于 Linux 的⾼性能⽹络程序都是使⽤ Reactor ⽅案。
⽽ Windows ⾥实现了⼀套完整的⽀持 socket 的异步编程接⼝，这套接⼝就是 IOCP ，是由操作系统级别实现的异步 I/O，真正意义上异步 I/O，因此在 Windows ⾥实现⾼性能⽹络程序可以使⽤效率更⾼的Proactor ⽅案。


## 内存管理
### 内存的物理结构
![](%E7%B3%BB%E7%BB%9F/1CF44745-ECF0-4E44-A3AC-705693907D8C.png)
内存的内部是由各种IC电路组成的。主要分为3种存储器：
* RAM 随机存储器
* ROM 只读存储器
* Cache高速缓存：分为L1 Cache、L2 Cache、L3 Cache，位于内存与CPU之间。
#### 内存IC（Integrated Circuit，集成电路）：
内存IC是一个完整的结构，内部也有电源、地址信号、数据信号、控制信号和用于寻址的IC引脚来进行数据的读写。
下图是一个虚拟的IC引脚示意图：
![](%E7%B3%BB%E7%BB%9F/E4051809-4574-44CF-A819-916CDE45231E.png)
D0～D7表示数据信号，一次可以输入输出8bit=1byte数据。
A0～A9是地址信号，可以指定00000 00000 - 11111 11111 共2^10 = 1024个地址。
因此内存IC的容量就是1KB。
通常，一个内存IC会有更多的引脚，也就能存储更多数据。
#### 内存的读写过程：
![](%E7%B3%BB%E7%BB%9F/780B6799-5828-4A30-BC74-070A00FECB2C.png)
#### 节约内存方式
1. 通过DLL文件实现函数共有：
多个应用可以共有同一个DLL文件，通过共有可以节约内存；
2. 通过调用_stdcall来减少程序文件的大小
_stdcall是standard call（标准调用）的缩写。Windows提供的DLL文件内的函数，基本上都是通过_stdcall调用方式来完成的。
C语言编写的程序默认都不是_stdcall，原因是因为C语言所对应的函数传入参数是可变的，只有函数调用方才能知道到底有多少个参数，这种情况下栈的清理作业便无法进行。不过若函数的参数和数量固定的话，指定_stdcall是可以的。
栈执行清理工作，在调用方法处执行和在反复调用方法处执行不同，使用_stdcall标准调用的方式称为反复调用方法，这种情况下开销比较小：
![](%E7%B3%BB%E7%BB%9F/07BCA33C-2615-41FB-BBBD-6131E3F682AA.png)

### 虚拟内存
![](%E7%B3%BB%E7%BB%9F/F288D8F3-033C-4BDA-B1C9-A7A771711E6A.png)
### 内存分段
程序是由若⼲个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就⽤分段（Segmentation）的形式把这些段分离出来。
![](%E7%B3%BB%E7%BB%9F/C2C4F5BA-93D5-4DC8-899F-0FBCE40C5868.png)
分段解决了程序本身不需要关⼼具体的物理内存地址的问题，好处就是能产⽣连续的内存空间，但它也有⼀些不⾜之处：
1）第⼀个就是内存碎⽚的问题。
2）第⼆个就是内存交换的效率低的问题。
### 内存分页
分⻚是把整个虚拟和物理内存空间切成⼀段段固定尺⼨的⼤⼩。这样⼀个连续并且尺⼨固定的内存空间， 我们叫⻚（Page）。在 Linux 下，每⼀⻚的⼤⼩为4KB 。
虚拟地址与物理地址之间通过⻚表来映射，如下图：
![](%E7%B3%BB%E7%BB%9F/561357CB-6447-40BD-ADF5-09C061AEC527.png)
⻚表是存储在内存⾥的，内存管理单元  （MMU）就做将虚拟内存地址转换成物理地址的⼯作。
⽽当进程访问的虚拟地址在⻚表中查不到时，系统会产⽣⼀个缺⻚异常，进⼊系统内核空间分配物理内存、更新进程⻚表，最后再返回⽤户空间，恢复进程的运⾏。
在分⻚机制下，虚拟地址分为两部分，⻚号和⻚内偏移。⻚号作为⻚表的索引，⻚表包含物理⻚每⻚所在物理内存的基地址，这个基地址与⻚内偏移的组合就形成了物理内存地址，⻅下图：
![](%E7%B3%BB%E7%BB%9F/F253438D-C2A7-474B-BE3A-6479AC192A9A.png)
简单的分⻚有的缺陷：空间上的缺陷。因为操作系统是可以同时运⾏⾮常多的进程的，那这不就意味着⻚表会⾮常的庞⼤。
##### 多级⻚表（Multi-Level Page Table）
在 32 位的环境下，虚拟地址空间共有 4GB，假设⼀个⻚的⼤⼩是 4KB（2^12），那么就需要⼤约 100 万（2^20）  个⻚，每个「⻚表项」需要 4 个字节⼤⼩来存储，那么整个 4GB 空间的映射就需要有4MB 的内存来存储⻚表。
把这个 100 多万个「⻚表项」的单级⻚表再分⻚，将⻚表（⼀级⻚表）分为     1024  个⻚表（⼆级⻚表），每个表（⼆级⻚表）中包含     1024  个「⻚表项」，形成⼆级分⻚。如下图所示：
![](%E7%B3%BB%E7%BB%9F/296FD820-1EC6-4B41-957B-CC079653DC97.png)
如果使⽤了⼆级分⻚，⼀级⻚表就可以覆盖整个 4GB 虚拟地址空间，但如果某个⼀级⻚表的⻚表项没有被⽤到，也就不需要创建这个⻚表项对应的⼆级⻚表了，即可以在需要时才创建⼆级⻚表。
保存在内存中的⻚表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在⻚表中找不到对应的⻚表项，计算机系统就不能⼯作了。所以⻚表⼀定要覆盖全部虚拟地址空间，不分级的⻚表就需要有 100 多万个⻚表项来映射，⽽⼆级分⻚则只需要 1024 个⻚表项（此时⼀级⻚表覆盖到了全部虚拟地址空间，⼆级⻚表在需要时创建）。
对于 64 位的系统，两级分⻚肯定不够了，就变成了四级⽬录，分别是：
        * 全局⻚⽬录项 PGD（Page Global Directory）； 
        * 上层⻚⽬录项 PUD（Page Upper Directory）； 
        * 中间⻚⽬录项 PMD（Page Middle Directory）； 
        * ⻚表项 PTE（Page Table Entry）；
![](%E7%B3%BB%E7%BB%9F/A82507EE-21A5-4684-9E3B-02F6EF496004.png)
##### TLB
TLB（Translation Lookaside Buffer）  ，通常称为⻚表缓存、转址旁路缓存、快表等。专⻔存放程序最常访问的⻚表项的 Cache。
![](%E7%B3%BB%E7%BB%9F/3ACFAA9D-1732-46BD-A026-8B87D1436F74.png)
### 段⻚式内存管理
段⻚式内存管理实现的⽅式：
				1. 先将程序划分为多个有逻辑意义的段，也就是前⾯提到的分段机制；
				2. 接着再把每个段划分为多个⻚，也就是对分段划分出来的连续空间，再划分固定⼤⼩的⻚；
这样，地址结构就由段号、段内⻚号和⻚内位移三部分组成。
⽤于段⻚式地址变换的数据结构是每⼀个程序⼀张段表，每个段⼜建⽴⼀张⻚表，段表中的地址是⻚表的起始地址，⽽⻚表中的地址则为某⻚的物理⻚号，如图所示：
![](%E7%B3%BB%E7%BB%9F/43A753A3-5D5E-44AE-82B0-CEF342723410.png)
段⻚式地址变换中要得到物理地址须经过三次内存访问：
        第⼀次访问段表，得到⻚表起始地址； 
        第⼆次访问⻚表，得到物理⻚号；
        第三次将物理⻚号与⻚内位移组合，得到物理地址。
### Linux内存管理
Linux 内存主要采⽤的是⻚式内存管理，但同时也不可避免地涉及了段机制。
这主要是上⾯ Intel 处理器发展历史导致的，因为 Intel X86 CPU ⼀律对程序中使⽤的地址先进⾏段式映射，然后才能进⾏⻚式映射。
Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是⼀样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应⽤程序代码，所⾯对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被⽤于访问控制和内存保护。
Linux上虚拟地址空间分布：
![](%E7%B3%BB%E7%BB%9F/F7E99B2A-DB26-4CC6-8621-F832AAF901C6.png)
用户空间分布：
![](%E7%B3%BB%E7%BB%9F/75F4B641-0C68-4C90-BFA2-DDB0D2C52433.png)



## 文件系统
### 文件系统的基本组成：
Linux ⽂件系统会为每个⽂件分配两个数据结构：索引节点（index node）和⽬录项（directory entry），它们主要⽤来记录⽂件的元信息和⽬录层次结构。
* 索引节点，也就是 inode，⽤来记录⽂件的元信息，⽐如 inode 编号、⽂件⼤⼩、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是⽂件的唯⼀标识，它们之间⼀⼀对应，也同样都会被存储在硬盘中，所以索引节点同样占⽤磁盘空间。
* ⽬录项，也就是 dentry，⽤来记录⽂件的名字、索引节点指针以及与其他⽬录项的层级关联关系。多个⽬录项关联起来，就会形成⽬录结构，但它与索引节点不同的是，⽬录项是由内核维护的⼀个数据结构，不存放于磁盘，⽽是缓存在内存。
**文件数据在磁盘上的存储：**
⽂件系统把多个扇区组成了⼀个逻辑块，每次读写的最⼩单位就是逻辑块（数据块），Linux 中的逻辑块⼤⼩为4KB ，也就是⼀次性读写 8 个扇区，这将⼤⼤提⾼了磁盘的读写的效率。
**索引节点、⽬录项以及⽂件数据的关系：**
![](%E7%B3%BB%E7%BB%9F/5CB62D12-D918-42EF-87C0-4B74B02B679F.png)

磁盘进⾏格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区：
* 超级块，⽤来存储⽂件系统的详细信息，⽐如块个数、块⼤⼩、空闲块等等。当⽂件系统挂载时进⼊内存； 
* 索引节点区，⽤来存储索引节点；当⽂件被访问时进⼊内存；
* 数据块区，⽤来存储⽂件或⽬录数据；

### 虚拟文件系统
在 Linux ⽂件系统中，⽤户空间、系统调⽤、虚拟机⽂件系统、缓存、⽂件系统以及存储之间的关系：
![](%E7%B3%BB%E7%BB%9F/4938C730-7F97-477E-8816-D788FBA6356A.png)

### 文件数据在磁盘上的存放方式
* 连续空间存放⽅式 ：「磁盘空间碎⽚」和「⽂件⻓度不易扩展」的缺陷
* ⾮连续空间存放⽅式：分为「链表⽅式」和「索引⽅式」。
![](%E7%B3%BB%E7%BB%9F/DB45EF47-D9C9-4536-B7B5-B4B078A5BAB3.png)
早期 Unix ⽂件系统的存放方式：
![](%E7%B3%BB%E7%BB%9F/F18CF51D-7D9D-4D79-93FB-6828CF6E53CE.png)
![](%E7%B3%BB%E7%BB%9F/14516644-E91A-42E6-A078-75B9F84A6762.png)
它是根据⽂件的⼤⼩，存放的⽅式会有所变化：
          * 如果存放⽂件所需的数据块⼩于 10 块，则采⽤直接查找的⽅式； 
          * 如果存放⽂件所需的数据块超过 10 块，则采⽤⼀级间接索引⽅式；
          * 如果前⾯两种⽅式都不够存放⼤⽂件，则采⽤⼆级间接索引⽅式； 
          * 如果⼆级间接索引也不够存放⼤⽂件，这采⽤三级间接索引⽅式；
那么，⽂件头（Inode）就需要包含 13 个指针：
          * 10 个指向数据块的指针；
          * 第 11 个指向索引块的指针； 
          * 第 12 个指向⼆级索引块的指针； 
          * 第 13 个指向三级索引块的指针；
所以，这种⽅式能很灵活地⽀持⼩⽂件和⼤⽂件的存放：
          * 对于⼩⽂件使⽤直接查找的⽅式可减少索引数据块的开销；
          * 对于⼤⽂件则以多级索引的⽅式来⽀持，所以⼤⽂件在访问数据块时需要⼤量查询；
这个⽅案就⽤在了 Linux Ext 2/3 ⽂件系统⾥，虽然解决⼤⽂件的存储，但是对于⼤⽂件的访问，需要⼤量的查询，效率⽐较低。
### 空闲空间管理⽅ 法：
* 空闲表法 
* 空闲链表法 
* 位图法
 Linux 是⽤位图的⽅式管理空闲空间，⽤户在创建⼀个新⽂件时，Linux 内核会通过 inode 的位图找到空闲可⽤的 inode，并进⾏分配。要存储数据时，会通过块的位图找到空闲的块，并分配。
### 文件系统的结构
Linux Ext2 整个⽂件系统的结构和块组的内容，⽂件系统都由⼤量块组组成，在硬盘上相继排布：
![](%E7%B3%BB%E7%BB%9F/CAFAF895-7CFE-4761-B54E-B4FCCAA55C85.png)
引导块：在系统启动时⽤于启⽤引导。
* 超级块：包含的是⽂件系统的重要信息，⽐如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。
* 块组描述符：包含⽂件系统中各个块组的状态，⽐如块组中空闲块和 inode 的数⽬等，每个块组都包含了⽂件系统中「所有块组的组描述符信息」。
* 数据位图和 inode 位图:：⽤于表示对应的数据块或 inode 是空闲的，还是被使⽤中。
* inode 列表：包含了块组中所有的 inode，inode ⽤于保存⽂件系统中与各个⽂件和⽬录相关的所有元数据。
* 数据块：包含⽂件的有⽤数据。
**每个块组⾥有很多重复的信息，⽐如超级块和块组描述符表，这两个都是全局信息，⽽且⾮常的重要**，这么做是有两个原因：
    1）如果系统崩溃破坏了超级块或块组描述符，有关⽂件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。
    2）通过使⽂件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提⾼⽂件系统的性能。
不过，Ext2 的后续版本采⽤了稀疏技术。该做法是，超级块和块组描述符表不再存储到⽂件系统的每个块组中，⽽是只写⼊到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。
### 目录的存储
![](%E7%B3%BB%E7%BB%9F/59791BFE-7A91-4F76-B9BF-82CBEFFA0ABF.png)
Linux 系统的 ext ⽂件系统就是采⽤了哈希表，来保存⽬录的内容，这种⽅法的优点是查找⾮常迅速，插⼊和删除也较简单，不过需要⼀些预备措施来避免哈希冲突。
⽬录查询是通过在磁盘上反复搜索完成，需要不断地进⾏ I/O 操作，开销较⼤。所以，为了减少 I/O 操作，把当前使⽤的⽂件⽬录缓存在内存，以后要使⽤该⽂件时只要在内存中操作，从⽽降低了磁盘操作次数，提⾼了⽂件系统的访问速度。
### 软链接和硬链接
硬链接是多个⽬录项中的「索引节点」指向⼀个⽂件，也就是指向同⼀个 inode，但是 inode 是不可能跨越⽂件系统的，每个⽂件系统都有各⾃的 inode 数据结构和列表，所以硬链接是不可⽤于跨⽂件系统的。 
由于多个⽬录项都是指向⼀个 inode，那么只有删除⽂件的所有硬链接以及源⽂件时，系统才会彻底删除该⽂件。
![](%E7%B3%BB%E7%BB%9F/6B3D87C6-D677-4804-869D-C86CBB3FDEFF.png)
软链接相当于重新创建⼀个⽂件，这个⽂件有独⽴的 inode，但是这个⽂件的内容是另外⼀个⽂件的路径，所以访问软链接的时候，实际上相当于访问到了另外⼀个⽂件，所以软链接是可以跨⽂件系统的，甚⾄⽬标⽂件被删除了，链接⽂件还是在的，只不过指向的⽂件找不到了⽽已。
![](%E7%B3%BB%E7%BB%9F/213E3867-1785-44C3-9C39-80B985160018.png)
### 文件I/O
##### 缓冲与⾮缓冲 I/O
⽂件操作的标准库是可以实现数据的缓存，那么根据「是否利⽤标准库缓冲」，可以把⽂件 I/O 分为缓冲 I/O 和⾮缓冲 I/O：
          * 缓冲 I/O，利⽤的是标准库的缓存实现⽂件的加速访问，⽽标准库再通过系统调⽤访问⽂件。 
          * ⾮缓冲 I/O，直接通过系统调⽤访问⽂件，不经过标准库缓存。
这⾥所说的「缓冲」特指标准库内部实现的缓冲。
##### 直接与⾮直接 I/O
我们都知道磁盘 I/O 是⾮常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调⽤后，会把⽤户数据拷⻉到内核中缓存起来，这个内核缓存空间也就是「⻚缓存」，只有当缓存满⾜某些条件的时候，才发起磁盘 I/O 的请求。
那么，根据「是否利⽤操作系统的缓存」，可以把⽂件 I/O 分为直接 I/O 与⾮直接 I/O：
           * 直接 I/O，不会发⽣内核缓存和⽤户程序之间数据复制，⽽是直接经过⽂件系统访问磁盘。 
           * ⾮直接 I/O，读操作时，数据从内核缓存中拷⻉给⽤户程序，写操作时，数据从⽤户程序拷⻉给内核缓存，再由内核决定什么时候写⼊数据到磁盘。
如果你在使⽤⽂件操作类的系统调⽤函数时，指定了 O_DIRECT  标志，则表示使⽤直接 I/O。如果没有设置过，默认使⽤的是⾮直接 I/O。
如果⽤了⾮直接 I/O 进⾏写数据操作，以下⼏种场景会触发内核缓存的数据写⼊磁盘：
           * 在调⽤  write  的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上； 
           * ⽤户主动调⽤     sync ，内核缓存会刷到磁盘上；
           * 当内存⼗分紧张，⽆法再分配⻚⾯时，也会把内核缓存的数据刷到磁盘上； 
           * 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；
##### 阻塞与⾮阻塞 I/O VS 同步与异步 I/O
阻塞等待的是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程。
阻塞 I/O：当⽤户程序执⾏read，线程会被阻塞，⼀直等到内核数据准备好，并把数据从内核缓冲区拷⻉到应⽤程序的缓冲区中，当拷⻉过程完成，   read  才会返回。如下图：
![](%E7%B3%BB%E7%BB%9F/824D6E76-BD61-4F20-8FB6-931A729F69E9.png)
⾮阻塞 I/O：⾮阻塞的 read 请求在数据未准备好的情况下⽴即返回，可以继续往下执⾏，此时应⽤程序不断轮询内核，直到数据准备好，内核将数据拷⻉到应⽤程序缓冲区， read 调⽤才可以获取到结果。如下图：
![](%E7%B3%BB%E7%BB%9F/51C871F8-EBA3-4790-991B-959260042517.png)
注意，这⾥最后⼀次 read 调⽤，获取数据的过程，是⼀个同步的过程，是需要等待的过程。这⾥的同步指的是内核态的数据拷⻉到⽤户程序的缓存区这个过程。

实际上，⽆论是阻塞 I/O、⾮阻塞 I/O，还是基于⾮阻塞 I/O 的多路复⽤都是同步调⽤。因为它们在 read 调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷⻉效率不⾼，read 调⽤就会在这个同步过程中等待⽐较⻓的时间。
⽽真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程都不⽤等待。 
当我们发起  aio_read  之后，就⽴即返回，内核⾃动将数据从内核空间拷⻉到应⽤程序空间，这个拷⻉过程同样是异步的，内核⾃动完成的，和前⾯的同步操作不⼀样，应⽤程序并不需要主动发起拷⻉动作。过程如下图：
![](%E7%B3%BB%E7%BB%9F/EE4BE05B-234A-4BC4-ACB2-2E881E1D400E.png)
**总结：**
![](%E7%B3%BB%E7%BB%9F/44C95E39-CE7D-4C9A-8324-61CD23DAB565.png)
I/O 是分为两个过程的：
1.  数据准备的过程
2.  数据从内核空间拷⻉到⽤户进程缓冲区的过程
阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，⽽⾮阻塞 I/O 和基于⾮阻塞 I/O 的多路复⽤只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。
异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。

## I/O
### I/O设备
I/O设备分为两种：块设备和字符设备。
##### 块设备
是一个能存储固定大小块信息的设备，支持以固定大小的块、扇区或群集读取和（可选）写入数据。每个块都有自己的物理地址。通常块大小在512～65536之间。所有传输的信息都会以连续的块为单位。与字符设备相比需要较少的引脚。
常见的块设备有：硬盘、蓝光光盘、USB盘。
缺点：基于给定固态存储器的块设备比基于相同类型的存储器的字节寻址要慢一些，因为必须在块的开头开始读取或写入。
##### 字符设备
字符设备以字符为单位发送或接收一个字符流，而不考虑任何块结构。
字符设备是不可寻址的，也没有任何寻道操作。
常见的字符设备：打印机、网络设备、鼠标、以及大多数与磁盘不同的设备。
### I/O软件原理
I/O软件设计的目标：
        * 设备独立性：与设备独立性密切相关的一个指标就是统一命名。
        * 错误处理
        * 同步和异步传输
        * 缓冲
        * 共享设备和独占设备
##### 控制I/O设备的方法
一共有三种控制I/O设备的方法：
        * 使用程序控制I/O
        * 使用中断驱动I/O
        * 使用DMA驱动I/O
**使用程序控制I/O**：
![](%E7%B3%BB%E7%BB%9F/751D875D-516F-474C-AEB6-4B1B68E2E730.png)
![](%E7%B3%BB%E7%BB%9F/807EAB90-56DC-4A8B-96DF-3C64399E0E7E.png)
**使用中断驱动I/O**：
在CPU等待I/O设备的同时，能够做其他事情，等到I/O设备完成后，它就会产生一个中断，这个中断会停止当前进程并保存当前状态：
![](%E7%B3%BB%E7%BB%9F/375F50B1-1BBB-48DB-9A3C-BA00F2C5120E.png)
![](%E7%B3%BB%E7%BB%9F/52C1E06F-E80B-4EFB-A4C5-04FB2228C441.png)
**使用DMA的I/O**：
![](%E7%B3%BB%E7%BB%9F/CB9B5508-F21B-4D7B-9B6F-16F79321F652.png)
### I/O层次结构
![](%E7%B3%BB%E7%BB%9F/97442523-CA1E-42E3-B6F2-E9815AB4D13D.png)
##### 中断处理程序
![](%E7%B3%BB%E7%BB%9F/E1BA6014-2D89-400C-BE8D-7DB998700C2A.png)
**中断处理方案**：
![](%E7%B3%BB%E7%BB%9F/9CF6114F-E8DF-495C-A896-3F6DAF28EB78.png)
**通用的中断处理程序的步骤**：
![](%E7%B3%BB%E7%BB%9F/46D217EF-41A4-44AA-9B98-F83813984882.png)
![](%E7%B3%BB%E7%BB%9F/BCF836F6-23ED-42D2-A279-27C5D0261227.png)
下面是一个嵌套中断的具体运行步骤：
![](%E7%B3%BB%E7%BB%9F/FA699D38-9AA0-4112-889F-259CEFA6D352.png)
##### 与设备无关的I/O软件：
![](%E7%B3%BB%E7%BB%9F/397C217F-DEDA-42C3-B404-4ED6E29AA6BB.png)
### 盘
##### RAID
RAID称为磁盘冗余阵列，利用虚拟化技术把多个硬盘结合在一起，成为一个或多个磁盘阵列组，目的是提升性能或数据冗余。
![](%E7%B3%BB%E7%BB%9F/4AA1FA6F-D57D-428B-8D54-8AD0EF981F47.png)
##### 磁盘格式化
磁盘刚被创建出来后，没有任何信息。磁盘在使用前必须经过低级格式化。
下面是一个扇区的格式：
![](%E7%B3%BB%E7%BB%9F/F3BD3AA1-558B-42F9-9189-3E17217D32DB.png)
![](%E7%B3%BB%E7%BB%9F/C3D2A894-BC0A-43E0-BBBE-4167B112D676.png)
![](%E7%B3%BB%E7%BB%9F/4D009BA4-5CEA-413B-983B-34538AFF991D.png)
![](%E7%B3%BB%E7%BB%9F/788CF6CB-47C2-40BF-861B-ECF3F9BAD185.png)
### 时钟
##### 时钟硬件
![](%E7%B3%BB%E7%BB%9F/05684D87-9A11-4B10-8F3C-789F11FA28A9.png)
##### 时钟软件
一般操作系统的不同，时钟软件的具体实现也不同，但一般都会包括如下几点：
![](%E7%B3%BB%E7%BB%9F/8635E0A5-6710-4BE0-860D-67CFAD354705.png)
##### 软定时器
![](%E7%B3%BB%E7%BB%9F/333383B9-2E8C-436F-AFD7-F4401C1D472B.png)